---
title: "AE 17:Hypothesis Testing"
---

In this application exercise, we will do hypothesis testing for the slope in the linear model.

## Packages

We will use **tidyverse** and **tidymodels** for data exploration and modeling, respectively, and the **openintro** package for the data, and the **knitr** package for formatting tables.

```{r}
#| label: load-packages
#| message: false
library(tidyverse)
library(tidymodels)
library(openintro)
library(knitr)
```

## Data

In this exercise we will be studying predictors of mean household income accross counties in the United States.
The contains results for 3142 counties' results from the 2019 American Community Survey (this is all counties at the time).
The data set `county_2019` is from the `openintro` package.
Go ahead and take a look at the data using `glimpse`; you can get more information with `?county_2019`!

```{r}

# add code here
```

Now, let's imagine we only had a very small subset of these data to work with.
That is, we only have data from a small subset of counties instead of from all counties.
While that is not the case in this data set, we could easily images cases/studies where this might be true.

In the code below, we set a seed then take a random sample of 25 counties from our `county_2019` data.

```{r}
set.seed(123)
counties_sample <- county_2019 |>
  sample_n(25)
```

**Question:** With so little information, can we draw super strong conclusions?

## Plot

**Task**: First, plot the relationship between mean household income `mean_household_income` and `mean_work_travel` for the full data set.

```{r}
county_2019 |>
  ggplot(aes(x = uninsured, y = mean_household_income)) + 
  geom_point() + 
  geom_smooth(method = "lm") 
```

**Task**: Next, plot the relationship between mean household income `mean_household_income` and `uninsured` for the small sample data set.

```{r}
counties_sample |>
  ggplot(aes(x = uninsured, y = mean_household_income)) + 
  geom_point() + 
  geom_smooth(method = "lm") 
```

## Inference with the small sample dataset

### Point estimate

First, compute the point estimate using the small sample dataset.

```{r}
#| label: observed-baby-fit
#| eval: true

observed_fit <- counties_sample |>
  specify(mean_household_income ~ uninsured) |>
  fit()

observed_fit
```

### Simulate the null distribution

Now, simulate the null distribution. We are testing $H_0: \beta_1=0$ versus the alternative $H_A: \beta_1\neq 0$.

```{r}
#| label: simulate-null-dist
#| eval: true
set.seed(123)
null_dist <- counties_sample |>
  specify(mean_household_income ~ uninsured) |>
  hypothesize(null = "independence") |>
  generate(reps = 1000, type = "permute") |>
  fit()
```

Now, we are going to visualize the null distribution. Note that it's centered at zero, because if the null were true and the true slope was in fact zero, we would expect noisy, imperfect estimates of the slope to wiggle around 0.

```{r}
#| label: plot-null-dist
null_dist |> 
  filter(term == "uninsured") |>
  ggplot(aes(x = estimate)) + 
  geom_histogram()
```

### Where does our actual point estimate fall under the null distribution?

Shade the $p$-value:

```{r}
#| label: shade-p-value
#| eval: true
visualize(null_dist) +
  shade_p_value(obs_stat = observed_fit, direction = "two-sided")
```

```{r}
#| label: compute-p-value
#| eval: true
null_dist |>
  get_p_value(obs_stat = observed_fit, direction = "two-sided")
```

**Interpretation**: if the null were true (the true slope was zero), then the probability of data as or more extreme than what we saw in about 10%. At a 5% discernibility level, we fail to reject the null. With the data we have, you can't discern with tremendously high confidence whether the null is true or not. We just don't know.
