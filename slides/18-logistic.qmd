---
title: "Logistic regression"
subtitle: "Lecture 18"
date: "2025-6-11"
format: 
  revealjs:
    output-file: 18-logistic-slides.html
    pdf-separate-fragments: true
auto-stretch: false
---

## While you wait... {.smaller}

```{r}
#| label: load-packages
#| message: false
#| echo: false
library(tidyverse)
library(tidymodels)
library(openintro)
library(fivethirtyeight)
library(palmerpenguins)
library(MASS)
hp_spam <- read_csv("data/hp-spam.csv")
todays_ae <- "ae-14-spam-filter"
```

```{r}
#| label: cartoon-settings
#| message: false
#| echo: false

set.seed(8675309)
n <- 50
x <- rnorm(n, mean = 0, sd = 1)
b0 <- 0  
b1 <- 5
prob <- 1 / (1 + exp(-(b0 + b1 * x)))
y <- rbinom(n, 1, prob)
fit <- glm(y ~ x, family = binomial)
seq_x = seq(1.2 * min(x), 1.2 * max(x), length.out = 500)
y_vals <- 1 / (1 + exp(-(fit$coefficients[1] + fit$coefficients[2] * seq_x)))

example_df = data.frame(round(x, 2),y)
example_df = example_df |> rename(x = round.x..2.)
df = example_df |> slice(1:10)
```

::: appex
-   Go to your `ae` project in RStudio.

-   Make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.

-   Click Pull to get today's application exercise file: *`{r} paste0(todays_ae, ".qmd")`*.

-   Wait till the you're prompted to work on the application exercise during class before editing the file.
:::

## Thus far...

We have been studying regression:

-   What combinations of data types have we seen?

-   What did the picture look like?

## Today: a *binary* response {.smaller}

Categorical with **two levels** (0 or 1).

. . .

::: incremental
-   Yes (1) vs. No (0)
-   Win (1) vs. Lose (0)
-   True (1) vs. False (0)
-   Heads (1) vs. Tails (0)
-   And so much more!
:::

. . .

$$
y = 
\begin{cases}
1 & &&\text{eg. Yes, Win, True, Heads, ...}\\
0 & &&\text{eg. No, Lose, False, Tails, ...}
\end{cases}
$$

## Example Plot

::: columns
::: {.column width="20%"}
```{r}
df
```
:::

::: {.column width="80%"}
```{r}
#| message: false
#| echo: false
par(mar = c(5, 5, 0.5, 0.5))
plot(x, y, pch = 19, cex = 1.3, ylim = c(-0.25, 1.25), cex.lab = 1.5,
     ylab = "y",
     xlab = "x")

```
:::
:::

## Who cares?

If we can model the relationship between predictors ($x$) and a binary response ($y$), we can use the model to do a special kind of prediction called **classification**.

## Example: is the e-mail spam or not? {.smaller}

$$
\mathbf{x}: \text{word and character counts in an e-mail.}
$$

![](images/18/spam.png){fig-align="center" width="70%"}

$$
y
= 
\begin{cases}
1 & \text{it's spam}\\
0 & \text{it's legit}
\end{cases}
$$

## Example: is it cancer or not? {.smaller}

$$
\mathbf{x}: \text{features in a medical image.}
$$

![](images/18/head-neck.jpg){fig-align="center"}

$$
y
= 
\begin{cases}
1 & \text{it's cancer}\\
0 & \text{it's healthy}
\end{cases}
$$

## Example: will they default? {.smaller}

$$
\mathbf{x}: \text{financial and demographic info about a loan applicant.}
$$

![](images/18/fico.jpg){fig-align="center" width="60%"}

$$
y
= 
\begin{cases}
1 & \text{applicant is at risk of defaulting on loan}\\
0 & \text{applicant is safe}
\end{cases}
$$

## How do we model this type of data? {.smaller}

```{r}
#| message: false
#| echo: false
par(mar = c(5, 5, 0.5, 0.5))
plot(x, y, pch = 19, cex =  1.3, ylim = c(-0.25, 1.25), cex.lab = 1.5,
     ylab = "y",
     xlab = "x")
```

## Straight line of best fit is a little silly {.smaller}

```{r}
#| message: false
#| echo: false
par(mar = c(5, 5, 0.5, 0.5))
plot(x, y, pch = 19, cex = 1.3, ylim = c(-0.25, 1.25), cex.lab = 1.5,
     ylab = "y",
     xlab = "x")
abline(lm(y ~ x), col = "hotpink2", lwd = 3)
```

## Modelling probabilities {.smaller}

Instead of modeling $y$ directly, let's model the probability that $y=1$:

-   "Given new email, what's the probability that it's spam?''
-   "Given new image, what's the probability that it's cancer?''
-   "Given new loan application, what's the probability that they default?''

## Modelling probabilities: lines are still silly {.smaller}

```{r}
#| message: false
#| echo: false
par(mar = c(5, 5, 0.5, 0.5))
plot(x, y, pch = 19, cex = 1.3, ylim = c(-0.25, 1.25), cex.lab = 1.5,
     ylab = "Prob(y = 1)",
     xlab = "x")
abline(lm(y ~ x), col = "hotpink2", lwd = 3)
```

## Instead: S-curve of best fit {.smaller}

```{r}
#| message: false
#| echo: false
par(mar = c(5, 5, 0.5, 0.5))
plot(x, y, pch = 19, cex = 1.3, ylim = c(-0.25, 1.25), cex.lab = 1.5,
     ylab = "Prob(y = 1)",
     xlab = "x")
lines(seq_x, y_vals, col = "hotpink2", lwd = 3)
points(x, y, pch = 19, cex = 0.75)
```

## Why don't we model y directly? {.smaller}

-   **Recall regression with a numerical response**:\
    \
    Our models do not output *guarantees* for $y$, they output predictions that describe behavior *on average*;

. . .

<br>

-   **Similar when modeling a binary response**:\
    \
    Our models cannot directly guarantee that $y$ will be zero or one. The correct analog to "on average" for a 0/1 response is "what's the probability?"

## On average vs. What's the probability? {.smaller}

Let's suppose I'm classifying emails as spam ( $y = 1$ ) vs. legit ( $y$ = 0 ).
At some given length (suppose $x = 500$ words), I see that:

-   8 emails were spam

-   2 emails were legit

. . .

What does it mean to average these together?

. . .

$$
\frac{1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 0 + 0}{10} = \frac{8}{10} = 0.8
$$

## Again: S-curve of best fit {.smaller}

```{r}
#| message: false
#| echo: false
par(mar = c(5, 5, 0.5, 0.5))
plot(x, y, pch = 19, cex = 1.3, ylim = c(-0.25, 1.25), cex.lab = 1.5,
     ylab = "Prob(y = 1)",
     xlab = "x")
lines(seq_x, y_vals, col = "hotpink2", lwd = 3)
points(x, y, pch = 19, cex = 0.75)
```

## So, what is this S-curve, anyways? {.smaller}

It's the *logistic function*:

$$
\text{Prob}(y = 1)
=
\frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}.
$$

. . .

If you set $p = \text{Prob}(y = 1)$ and do some algebra, you get the simple linear model for the *log-odds*:

. . .

$$
\log\left(\frac{p}{1-p}\right)
=
\beta_0+\beta_1x.
$$

. . .

This is called the *logistic regression* model.

## Log-odds? {.smaller}

::: incremental
-   $p = \text{Prob}(y = 1)$ is a probability.
    A number between 0 and 1;

-   $\frac{p}{(1 - p)}$ is the odds.
    A number between 0 and $\infty$;

    -   80% probability an email is spam; 20% an email is legit

    -   The odds an email is spam are 4 to 1

-   The log odds $\log(\frac{p}{1 - p})$ is a number between $-\infty$ and $\infty$, which is suitable for the linear model.
:::

. . .

$$
\log\left(\frac{p}{1-p}\right)
=
\beta_0+\beta_1x.
$$

## Probability to odds

```{r}
#| echo: false
#| fig-asp: 0.6

curve(x/(1 - x), from = 0, to = 1, n = 500,
      xlab = "p", ylab = "p / (1 - p)",
      lwd = 2, col = "red", bty = "n")
```

## Odds to log odds

```{r}
#| echo: false
#| fig-asp: 0.6

curve(log(x), from = 0, to = 10, n = 1000,
      lwd = 2, col = "red", xaxt = "n", bty = "n",
      xlab = "p / (1 - p)", ylab = "log[p / (1 - p)]")
axis(1, pos = 0)
```

## Logistic regression {.smaller}

$$
\log\left(\frac{p}{1-p}\right)
=
\beta_0+\beta_1x.
$$

::: incremental
-   The *logit* function $\log(\frac{p}{1-p})$ is an example of a *link function* that transforms the linear model to have an appropriate range;

-   This is an example of a *generalized linear model*
:::

## Estimation {.smaller}

::: incremental
-   We estimate the parameters $\beta_0,\,\beta_1$ using *maximum likelihood* (don't worry about it) to get the "best fitting" S-curve;

-   The fitted model is
:::

. . .

$$
\log\left(\frac{\widehat{p}}{1-\widehat{p}}\right)
=
b_0+b_1x.
$$

## Today's data {.smaller}

```{r}
#| echo: false
email |> dplyr::select(c(spam, dollar, viagra, winner, password, exclaim_mess)) 
```

## Fitting a logistic model {.smaller}

```{r}
logistic_fit <- logistic_reg() |>
  fit(spam ~ exclaim_mess, data = email)

tidy(logistic_fit)
```

## Fitting a logistic model {.smaller}

```{r}
tidy(logistic_fit)
```

. . .

Fitted equation for the log-odds:

$$
\log\left(\frac{\hat{p}}{1-\hat{p}}\right)
=
-2.27
+
0.000272\times exclaim~mess
$$

## Be careful!! {.smaller}

üíñ‚úÖ This is correct‚úÖüíñ

$$
\log\left(\frac{\hat{p}}{1-\hat{p}}\right)
=
-2.27
+
0.000272\times exclaim~mess
$$

<br>

‚ùå üõëThese are wrong!
Do not do this!
‚ùåüõë

$$
\widehat{spam}
=
-2.27
+
0.000272\times exclaim~mess
$$

$$
\widehat{p}
=
-2.27
+
0.000272\times exclaim~mess
$$

## Interpreting the intercept {.smaller}

Plug in $x = 0$:

$$
\log\left(\frac{\widehat{p}}{1-\widehat{p}}\right)
=
b_0+b_1x.
$$

. . .

When $x = 0$, the estimated probability that $y = 1$ is

$$
\hat{p} = \frac{e^{b_0}}{1+e^{b_0}}
$$

## Interpreting the intercept: emails {.smaller}

If `exclaim_mess = 0`, then

$$
\hat{p}=\widehat{P(y=1)}=\frac{e^{-2.27}}{1+e^{-2.27}}\approx 0.09.
$$

So, we estimate that an email with no exclamation marks has a 9% chance of being spam.

## Interpreting the slope is tricky {.smaller}

Recall:

$$
\log\left(\frac{\widehat{p}}{1-\widehat{p}}\right)
=
b_0+b_1x.
$$

. . .

Alternatively:

$$
\frac{\widehat{p}}{1-\widehat{p}}
=
e^{b_0+b_1x}
=
\color{blue}{e^{b_0}e^{b_1x}}
.
$$

. . .

If we increase $x$ by one unit, we have:

$$
\frac{\widehat{p}}{1-\widehat{p}}
=
e^{b_0}e^{b_1(x+1)}
=
e^{b_0}e^{b_1x+b_1}
=
{\color{blue}{e^{b_0}e^{b_1x}}}{\color{red}{e^{b_1}}}
.
$$

. . .

A one unit increase in $x$ is associated with a change in odds by a factor of $e^{b_1}$.
Gross!

## Sign of the slope is meaningful {.smaller}

A one unit increase in $x$ is associated with a change in odds by a factor of $e^{b_1}$.

. . .

-   A positive slope means increasing $x$ increases the odds (and probability!) that $y = 1$
-   A negative slope means increasing $x$ decreases the odds (and probability!) that $y = 1$

## Back to the example... {.smaller}

$$
\log\left(\frac{\hat{p}}{1-\hat{p}}\right)
=
-2.27
+
0.000272\times exclaim~mess
$$

If we add one exclamation mark to the model, we predict the odds of an email being spam to be **higher** by a **factor** of $e^{0.000272}\approx 1.000272$ on average.

# Logistic regression -\> classification?

## Step 0: fit the model {.smaller}

```{r}
#| label: step-0
#| message: false
#| echo: false
#| fig-asp: 0.45
threshold <- 0.7
cutoff <- (log(threshold / (1 - threshold)) - fit$coefficients[1]) / fit$coefficients[2]
par(mar = c(5, 10, 0.5, 0.5))
plot(x, y, pch = 19, cex = 0.5, ylim = c(-0.25, 1.25), cex.lab = 1.5,
     #xlim = c(1.2 * min(x), 1.2 * max(x)),
     ylab = "",
     xlab = "")
lines(seq_x, y_vals, col = "red", lwd = 3)
points(x, y, pch = 19, cex = 0.75)
mtext("Prob(y = 1)", side = 2, line = 8, at = 0.5, cex = 2)
mtext("x", side = 1, line = 4, at = 0, cex = 2)
#mtext(paste("p* =", threshold), side = 2, line = 1, las = 2, at = threshold, cex = 1.5)
#segments(1.2 * min(x), threshold, cutoff, threshold, lty = 2, lwd = 2)
```

## Step 1: pick a threshold {.smaller}

Select a number $0 < p^* < 1$:

```{r}
#| label: step-1
#| message: false
#| echo: false
#| fig-asp: 0.45

cutoff <- (log(threshold / (1 - threshold)) - fit$coefficients[1]) / fit$coefficients[2]
par(mar = c(5, 10, 0.5, 0.5))
plot(x, y, pch = 19, cex = 0.5, ylim = c(-0.25, 1.25), cex.lab = 1.5,
     #xlim = c(1.2 * min(x), 1.2 * max(x)),
     ylab = "",
     xlab = "")
lines(seq_x, y_vals, col = "red", lwd = 3)
points(x, y, pch = 19, cex = 0.75)
mtext("Prob(y = 1)", side = 2, line = 8, at = 0.5, cex = 2)
mtext("x", side = 1, line = 4, at = 0, cex = 2)
mtext(paste("p* =", threshold), side = 2, line = 1, las = 2, at = threshold, cex = 1.5)
segments(1.2 * min(x), threshold, cutoff, threshold, lty = 2, lwd = 2)
```

-   if $\text{Prob}(y=1)\leq p^*$, then predict $\widehat{y}=0$;
-   if $\text{Prob}(y=1)> p^*$, then predict $\widehat{y}=1$.

## Step 2: find the "decision boundary" {.smaller}

Solve for the x-value that matches the threshold:

```{r}
#| label: step-2
#| message: false
#| echo: false
#| fig-asp: 0.45

cutoff <- (log(threshold / (1 - threshold)) - fit$coefficients[1]) / fit$coefficients[2]
par(mar = c(5, 10, 0.5, 0.5))
plot(x, y, pch = 19, cex = 0.5, ylim = c(-0.25, 1.25), cex.lab = 1.5,
     #xlim = c(1.2 * min(x), 1.2 * max(x)),
     ylab = "",
     xlab = "")
lines(seq_x, y_vals, col = "red", lwd = 3)
points(x, y, pch = 19, cex = 0.75)
mtext("Prob(y = 1)", side = 2, line = 8, at = 0.5, cex = 2)
mtext("x", side = 1, line = 4, at = 0, cex = 2)
mtext(paste("p* =", threshold), side = 2, line = 1, las = 2, at = threshold, cex = 1.5)
segments(1.2 * min(x), threshold, cutoff, threshold, lty = 2, lwd = 2)
abline(v = cutoff, lty = 2, lwd = 2)
mtext("x*", side = 1, line = 1, at = cutoff, cex = 2)
```

-   if $\text{Prob}(y=1)\leq p^*$, then predict $\widehat{y}=0$;
-   if $\text{Prob}(y=1)> p^*$, then predict $\widehat{y}=1$.

## Step 3: classify a new arrival {.smaller}

A new person shows up with $x_{\text{new}}$.
Which side of the boundary are they on?

```{r}
#| label: step-3
#| message: false
#| echo: false
#| fig-asp: 0.45

cutoff <- (log(threshold / (1 - threshold)) - fit$coefficients[1]) / fit$coefficients[2]
par(mar = c(5, 10, 0.5, 0.5))
plot(x, y, pch = 19, cex = 0.5, ylim = c(-0.25, 1.25), cex.lab = 1.5,
     #xlim = c(1.2 * min(x), 1.2 * max(x)),
     ylab = "",
     xlab = "")
lines(seq_x, y_vals, col = "red", lwd = 3)
points(x, y, pch = 19, cex = 0.75)
mtext("Prob(y = 1)", side = 2, line = 8, at = 0.5, cex = 2)
mtext("x", side = 1, line = 4, at = 0, cex = 2)
mtext(paste("p* =", threshold), side = 2, line = 1, las = 2, at = threshold, cex = 1.5)
segments(1.2 * min(x), threshold, cutoff, threshold, lty = 2, lwd = 2)
abline(v = cutoff, lty = 2, lwd = 2)
mtext("x*", side = 1, line = 1, at = cutoff, cex = 2)
polygon(c(-20, cutoff, cutoff, -20), c(-1, -1, 2, 2), col = rgb(1, .5, 0, alpha = 0.2), border = NA)
polygon(c(cutoff, 20, 20, cutoff), c(-1, -1, 2, 2), col = rgb(0, 0, 1, alpha = 0.2), border = NA)
text(-1, 0.5, expression(hat(y)~" = 0"), col = "orange", cex = 2)
text(1, 0.5, expression(hat(y)~" = 1"), col = "blue", cex = 2)
```

-   if $x_{\text{new}} \leq x^\star$, then $\text{Prob}(y=1)\leq p^*$, so predict $\widehat{y}=0$ for the new person;
-   if $x_{\text{new}} > x^\star$, then $\text{Prob}(y=1)> p^*$, so predict $\widehat{y}=1$ for the new person.

## Let's change the threshold {.smaller}

A new person shows up with $x_{\text{new}}$.
Which side of the boundary are they on?

```{r}
#| label: lower-threshold
#| message: false
#| echo: false
#| fig-asp: 0.45

threshold <- 0.15
cutoff <- (log(threshold / (1 - threshold)) - fit$coefficients[1]) / fit$coefficients[2]
par(mar = c(5, 10, 0.5, 0.5))
plot(x, y, pch = 19, cex = 0.5, ylim = c(-0.25, 1.25), cex.lab = 1.5,
     #xlim = c(1.2 * min(x), 1.2 * max(x)),
     ylab = "",
     xlab = "")
lines(seq_x, y_vals, col = "red", lwd = 3)
points(x, y, pch = 19, cex = 0.75)
mtext("Prob(y = 1)", side = 2, line = 8, at = 0.5, cex = 2)
mtext("x", side = 1, line = 4, at = 0, cex = 2)
mtext(paste("p* =", threshold), side = 2, line = 1, las = 2, at = threshold, cex = 1.5)
segments(1.2 * min(x), threshold, cutoff, threshold, lty = 2, lwd = 2)
abline(v = cutoff, lty = 2, lwd = 2)
mtext("x*", side = 1, line = 1, at = cutoff, cex = 2)
polygon(c(-20, cutoff, cutoff, -20), c(-1, -1, 2, 2), col = rgb(1, .5, 0, alpha = 0.2), border = NA)
polygon(c(cutoff, 20, 20, cutoff), c(-1, -1, 2, 2), col = rgb(0, 0, 1, alpha = 0.2), border = NA)
text(-1, 0.5, expression(hat(y)~" = 0"), col = "orange", cex = 2)
text(1, 0.5, expression(hat(y)~" = 1"), col = "blue", cex = 2)
```

-   if $x_{\text{new}} \leq x^\star$, then $\text{Prob}(y=1)\leq p^*$, so predict $\widehat{y}=0$ for the new person;
-   if $x_{\text{new}} > x^\star$, then $\text{Prob}(y=1)> p^*$, so predict $\widehat{y}=1$ for the new person.

## Let's change the threshold {.smaller}

A new person shows up with $x_{\text{new}}$.
Which side of the boundary are they on?

```{r}
#| label: higher-threshold
#| message: false
#| echo: false
#| fig-asp: 0.45

threshold <- 0.9
cutoff <- (log(threshold / (1 - threshold)) - fit$coefficients[1]) / fit$coefficients[2]
par(mar = c(5, 10, 0.5, 0.5))
plot(x, y, pch = 19, cex = 0.5, ylim = c(-0.25, 1.25), cex.lab = 1.5,
     #xlim = c(1.2 * min(x), 1.2 * max(x)),
     ylab = "",
     xlab = "")
lines(seq_x, y_vals, col = "red", lwd = 3)
points(x, y, pch = 19, cex = 0.75)
mtext("Prob(y = 1)", side = 2, line = 8, at = 0.5, cex = 2)
mtext("x", side = 1, line = 4, at = 0, cex = 2)
mtext(paste("p* =", threshold), side = 2, line = 1, las = 2, at = threshold, cex = 1.5)
segments(1.2 * min(x), threshold, cutoff, threshold, lty = 2, lwd = 2)
abline(v = cutoff, lty = 2, lwd = 2)
mtext("x*", side = 1, line = 1, at = cutoff, cex = 2)
polygon(c(-20, cutoff, cutoff, -20), c(-1, -1, 2, 2), col = rgb(1, .5, 0, alpha = 0.2), border = NA)
polygon(c(cutoff, 20, 20, cutoff), c(-1, -1, 2, 2), col = rgb(0, 0, 1, alpha = 0.2), border = NA)
text(-1, 0.5, expression(hat(y)~" = 0"), col = "orange", cex = 2)
text(1, 0.5, expression(hat(y)~" = 1"), col = "blue", cex = 2)
```

-   if $x_{\text{new}} \leq x^\star$, then $\text{Prob}(y=1)\leq p^*$, so predict $\widehat{y}=0$ for the new person;
-   if $x_{\text{new}} > x^\star$, then $\text{Prob}(y=1)> p^*$, so predict $\widehat{y}=1$ for the new person.

## Nothing special about one predictor... {.smaller}

Two numerical predictors and one binary response:

```{r}
#| message: false
#| echo: false
#| fig-asp: 0.45

set.seed(20)
n <- 20
cloud1 <- mvrnorm(n, c(-0.5, -0.5) + 4, matrix(c(1, 0.5, 0.5, 1), 2, 2))
cloud2 <- mvrnorm(n, c(0.5, 0.5) + 4, matrix(c(1, -0.5, -0.5, 1), 2, 2))
X <- rbind(cloud1, cloud2)
y <- c(rep(0, n), rep(1, n))
par(mar = c(5, 5, 0.5, 0.5))
plot(X, 
     pch = 19, 
     col = c(rep("orange", n), rep("blue", n)),
     xlab = expression(x[1]),
     ylab = expression(x[2]),
     xlim = c(-4, 4) + 4,
     ylim = c(-4, 4) + 4,
     cex.lab = 2)
legend("bottomright", c("y = 0", "y = 1"), pch = 19, col = c("orange", "blue"), bty = "n", cex = 2)
```

## "Multiple" logistic regression {.smaller}

On the probability scale:

$$
\text{Prob}(y = 1)
=
\frac{e^{\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_mx_m}}{1+e^{\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_mx_m}}.
$$

. . .

For the log-odds, a *multiple* linear regression:

$$
\log\left(\frac{p}{1-p}\right)
=
\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_mx_m.
$$

## Decision boundary, again {.smaller}

Consider two numerical predictors:

```{r}
#| message: false
#| echo: false
#| fig-asp: 0.45
fit <- glm(y ~ X, family = binomial)

b0 <- fit$coefficients[1]
b1 <- fit$coefficients[2]
b2 <- fit$coefficients[3]

p_thresh <- 0.5

# compute intercept and slope of decision boundary
bd_incpt <- (log(p_thresh / (1 - p_thresh)) - b0) / b2
bd_slp <- -b1 / b2

x_for_poly <- seq(0, 10, length.out = 500)
y_for_poly <- bd_incpt + bd_slp * x_for_poly

par(mar = c(4, 5, 0.5, 0.5))
plot(X, 
     pch = 19, 
     col = c(rep("orange", n), rep("blue", n)),
     xlab = expression(x[1]),
     ylab = expression(x[2]),
     xlim = c(-4, 4) + 4,
     ylim = c(-4, 4) + 4,
     cex.lab = 2)
abline(a = bd_incpt, b = bd_slp, lty = 2, lwd = 3)
polygon(c(x_for_poly, rev(x_for_poly)), c(y_for_poly, rep(10, 500)), col = rgb(0, 0, 1, alpha = 0.2), border = NA)
polygon(c(x_for_poly, rev(x_for_poly)), c(y_for_poly, rep(0, 500)), col = rgb(1, .5, 0, alpha = 0.2), border = NA)
legend("left", expression(hat(y)~" = 0"), text.col = "orange", bty = "n", cex = 2)
legend("right", expression(hat(y)~" = 1"), text.col = "blue", bty = "n", cex = 2)
points(X, pch = 19, 
     col = c(rep("orange", n), rep("blue", n)))
legend("topright", "p* = 0.5", cex = 2, bty = "n")
```

-   if new $(x_1,\,x_2)$ below, $\text{Prob}(y=1)\leq p^*$. Predict $\widehat{y}=0$ for the new person;
-   if new $(x_1,\,x_2)$ above, $\text{Prob}(y=1)> p^*$. Predict $\widehat{y}=1$ for the new person.

## Decision boundary, again {.smaller}

It's linear!
Consider two numerical predictors:

```{r}
#| message: false
#| echo: false
#| fig-asp: 0.45


fit <- glm(y ~ X, family = binomial)

b0 <- fit$coefficients[1]
b1 <- fit$coefficients[2]
b2 <- fit$coefficients[3]

p_thresh <- 0.15

# compute intercept and slope of decision boundary
bd_incpt <- (log(p_thresh / (1 - p_thresh)) - b0) / b2
bd_slp <- -b1 / b2

x_for_poly <- seq(0, 10, length.out = 500)
y_for_poly <- bd_incpt + bd_slp * x_for_poly

par(mar = c(4, 5, 0.5, 0.5))
plot(X, 
     pch = 19, 
     col = c(rep("orange", n), rep("blue", n)),
     xlab = expression(x[1]),
     ylab = expression(x[2]),
     xlim = c(-4, 4) + 4,
     ylim = c(-4, 4) + 4,
     cex.lab = 2)
abline(a = bd_incpt, b = bd_slp, lty = 2, lwd = 3)
polygon(c(x_for_poly, rev(x_for_poly)), c(y_for_poly, rep(10, 500)), col = rgb(0, 0, 1, alpha = 0.2), border = NA)
polygon(c(x_for_poly, rev(x_for_poly)), c(y_for_poly, rep(0, 500)), col = rgb(1, .5, 0, alpha = 0.2), border = NA)
legend("left", expression(hat(y)~" = 0"), text.col = "orange", bty = "n", cex = 2)
legend("right", expression(hat(y)~" = 1"), text.col = "blue", bty = "n", cex = 2)
points(X, pch = 19, 
     col = c(rep("orange", n), rep("blue", n)))
legend("topright", "p* = 0.15", cex = 2, bty = "n")
```

-   if new $(x_1,\,x_2)$ below, $\text{Prob}(y=1)\leq p^*$. Predict $\widehat{y}=0$ for the new person;
-   if new $(x_1,\,x_2)$ above, $\text{Prob}(y=1)> p^*$. Predict $\widehat{y}=1$ for the new person.

## Decision boundary, again {.smaller}

It's linear!
Consider two numerical predictors:

```{r}
#| message: false
#| echo: false
#| fig-asp: 0.45


fit <- glm(y ~ X, family = binomial)

b0 <- fit$coefficients[1]
b1 <- fit$coefficients[2]
b2 <- fit$coefficients[3]

p_thresh <- 0.9

# compute intercept and slope of decision boundary
bd_incpt <- (log(p_thresh / (1 - p_thresh)) - b0) / b2
bd_slp <- -b1 / b2

x_for_poly <- seq(0, 10, length.out = 500)
y_for_poly <- bd_incpt + bd_slp * x_for_poly

par(mar = c(4, 5, 0.5, 0.5))
plot(X, 
     pch = 19, 
     col = c(rep("orange", n), rep("blue", n)),
     xlab = expression(x[1]),
     ylab = expression(x[2]),
     xlim = c(-4, 4) + 4,
     ylim = c(-4, 4) + 4,
     cex.lab = 2)
abline(a = bd_incpt, b = bd_slp, lty = 2, lwd = 3)
polygon(c(x_for_poly, rev(x_for_poly)), c(y_for_poly, rep(10, 500)), col = rgb(0, 0, 1, alpha = 0.2), border = NA)
polygon(c(x_for_poly, rev(x_for_poly)), c(y_for_poly, rep(0, 500)), col = rgb(1, .5, 0, alpha = 0.2), border = NA)
legend("left", expression(hat(y)~" = 0"), text.col = "orange", bty = "n", cex = 2)
legend("right", expression(hat(y)~" = 1"), text.col = "blue", bty = "n", cex = 2)
points(X, pch = 19, 
     col = c(rep("orange", n), rep("blue", n)))
legend("topright", "p* = 0.9", cex = 2, bty = "n")
```

-   if new $(x_1,\,x_2)$ below, $\text{Prob}(y=1)\leq p^*$. Predict $\widehat{y}=0$ for the new person;
-   if new $(x_1,\,x_2)$ above, $\text{Prob}(y=1)> p^*$. Predict $\widehat{y}=1$ for the new person.

## Note: the classifier isn't perfect {.smaller}

```{r}
#| message: false
#| echo: false
#| fig-asp: 0.45


fit <- glm(y ~ X, family = binomial)

b0 <- fit$coefficients[1]
b1 <- fit$coefficients[2]
b2 <- fit$coefficients[3]

p_thresh <- 0.5

# compute intercept and slope of decision boundary
bd_incpt <- (log(p_thresh / (1 - p_thresh)) - b0) / b2
bd_slp <- -b1 / b2

x_for_poly <- seq(0, 10, length.out = 500)
y_for_poly <- bd_incpt + bd_slp * x_for_poly

par(mar = c(4, 5, 0.5, 0.5))
plot(X, 
     pch = 19, 
     col = c(rep("orange", n), rep("blue", n)),
     xlab = expression(x[1]),
     ylab = expression(x[2]),
     xlim = c(-4, 4) + 4,
     ylim = c(-4, 4) + 4,
     cex.lab = 2)
abline(a = bd_incpt, b = bd_slp, lty = 2, lwd = 3)
polygon(c(x_for_poly, rev(x_for_poly)), c(y_for_poly, rep(10, 500)), col = rgb(0, 0, 1, alpha = 0.2), border = NA)
polygon(c(x_for_poly, rev(x_for_poly)), c(y_for_poly, rep(0, 500)), col = rgb(1, .5, 0, alpha = 0.2), border = NA)
legend("left", expression(hat(y)~" = 0"), text.col = "orange", bty = "n", cex = 2)
legend("right", expression(hat(y)~" = 1"), text.col = "blue", bty = "n", cex = 2)
points(X, pch = 19, 
     col = c(rep("orange", n), rep("blue", n)))
legend("topright", "p* = 0.5", cex = 2, bty = "n")
```

-   Blue points in the orange region: spam (1) emails misclassified as legit (0);
-   Orange points in the blue region: legit (0) emails misclassified as spam (1).

## How do you pick the threshold? {.smaller}

To balance out the two kinds of errors:

![](images/18/confusion-matrix.png)

::: incremental
-   High threshold:
    -   Hard to classify as 1, so FP less likely and FN more likely
-   Low threshold:
    -    Easy to classify as 1, so FP more likely and FN less likely
:::

## Silly examples {.smaller}

-   Set p\* = 0

    -   Classify every email as spam (1);
    -   No false negatives, but *a lot* of false positives;

-   Set p\* = 1

    -   Classify every email as legit (0);
    -   No false positives, but *a lot* of false negatives.

You pick a threshold in between to strike a balance.
The exact number depends on context.

## `{r} todays_ae`

::: appex
-   Go to your ae project in RStudio.

-   If you haven't yet done so, make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.

-   If you haven't yet done so, click Pull to get today's application exercise file: *`{r} paste0(todays_ae, ".qmd")`*.

-   Work through the application exercise in class, and render, commit, and push your edits.
:::
