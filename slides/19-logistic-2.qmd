---
title: "More logistic regression"
subtitle: "Lecture 19"
date: "2025-6-19"
format: 
  revealjs:
    output-file: 19-logistic-2-slides.html
    pdf-separate-fragments: true
auto-stretch: false
---

## While you wait... {.smaller}

```{r}
#| label: load-packages
#| message: false
#| echo: false
library(tidyverse)
library(tidymodels)
library(openintro)
library(fivethirtyeight)
library(palmerpenguins)
library(MASS)
library(datasauRus)
library(forested)
hp_spam <- read_csv("data/hp-spam.csv")
todays_ae <- "ae-14-spam-filter"
```

```{r}
#| label: cartoon-settings
#| message: false
#| echo: false

set.seed(8675309)
n <- 50
x <- rnorm(n, mean = 0, sd = 1)
b0 <- 0  
b1 <- 5
prob <- 1 / (1 + exp(-(b0 + b1 * x)))
y <- rbinom(n, 1, prob)
fit <- glm(y ~ x, family = binomial)
seq_x = seq(1.2 * min(x), 1.2 * max(x), length.out = 500)
y_vals <- 1 / (1 + exp(-(fit$coefficients[1] + fit$coefficients[2] * seq_x)))

example_df = data.frame(round(x, 2),y)
example_df = example_df |> rename(x = round.x..2.)
df = example_df |> slice(1:10)
```

::: appex
-   Go to your `ae` project in RStudio.

-   Make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.

-   Click Pull to get today's application exercise file: *`{r} paste0(todays_ae, ".qmd")`*.

-   Wait till the you're prompted to work on the application exercise during class before editing the file.
:::

## Announcements

-   Office hours tomorrow will be about 20 mins short (1:00 - 2:40).
    I will be there a little before 1:00 if you want to come early!

-   Make appointment to see in-class exams/talk through problems you're stuck on.
    Remember, final exam is cumulative!!

## Project Reminders

-   Milestone 3 due Friday night!!

-   Once your project website renders, **you still need to push** for the changes to show up!!!

## Logistic Regression: Recap {.smaller}

*Goal*: modelling/predicting ***binary*** outcome $y$

-   Idea: Model the probability $p$ that $y = 1$

. . .

-   S-curve for the probability of success $p=P(y=1)$:

$$
    \hat{p} = \frac{e^{b_0+b_1x}}{1+e^{b_0+b_1x}}.
   $$

. . .

-   Linear model for the log-odds:

$$
    \log\left(\frac{\hat{p}}{1-\hat{p}}\right) = b_0+b_1x.
  $$

## R syntax is mostly unchanged {.smaller}

```{r}
simple_logistic_fit <- logistic_reg() |>
  fit(spam ~ exclaim_mess, data = email)

tidy(simple_logistic_fit)
```

. . .

Fitted equation for the log-odds:

$$
\log\left(\frac{\hat{p}}{1-\hat{p}}\right)
=
-2.27
+
0.000272\times exclaim~mess
$$

. . .

Interpretations are strange and delicate.

## Questions??? Review??? Anything?! {.smaller}

## Let's Practice

AE-14: Watch now, fill in later (optional!)

## Goal {.smaller}

-   We will build a spam filter from email data
-   The data come from incoming emails in David Diez's (one of the authors of OpenIntro textbooks) Gmail account for the first three months of 2012
-   User number of exclamation marks in an email address (`exclaim_mess`) to predict whether or not the email is spam (`spam`: 1 if spam; 0 if not)

## Data {.smaller}

```{r}
glimpse(email)
```

## EDA (Ex. 1) {.smaller}

```{r}
#| echo: false

ggplot(email, aes(x = exclaim_mess, fill = spam)) + 
  geom_density()
```

## EDA (Ex. 1) {.smaller}

Mean exclamation points by spam/not spam:

```{r}
#| echo: false

email |>
  group_by(spam) |>
  summarize(mean_ep = mean(exclaim_mess))
```

## Linear Model (Ex. 2) {.smaller}

```{r}
#| echo: false
#| message: false

ggplot(email, aes(x = exclaim_mess, y = as.numeric(spam) - 1)) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(
    x = "# of exclamation marks in email",
    y = "e-mail type"
  ) +
  scale_y_continuous(breaks = c(0, 1),
                   labels = c("legit (0)", "spam (1)"))
```

This makes no sense at all.

## Fit Logistic Regression (Ex. 3) {.smaller}

This is what we have seen already!

```{r}
spam_exl_fit <- logistic_reg() |>
  fit(spam ~ exclaim_mess, data = email)

tidy(spam_exl_fit)
```

$$
\log\left(\frac{\hat{p}}{1-\hat{p}}\right)
=
-2.27
+
0.000272\times exclaim~mess
$$

## Goal: Predict!! (Ex. 4a) {.smaller}

*What is the probability the email is spam if it contains 10 exclamation points??* Use `predict`!

```{r}
new_email <- tibble(
  exclaim_mess = 10
  )

new_email
```

## Goal: Predict!! (Ex. 4a) {.smaller}

*What is the probability the email is spam if it contains 10 exclamation points??* Use `predict`!

```{r}
new_email <- tibble(
  exclaim_mess = 10
  )

predict(spam_exl_fit, new_data = new_email, type = "prob")
```

## Goal: Predict!! (Ex. 4b) {.smaller}

*A probability is nice, but we want an actual decision. Classify the email!!!*

```{r}
predict(spam_exl_fit, new_data = new_email, type = "class")
```

. . .

The default behavior is to threshold the probabilities by 0.5.

## Let's use more data (Ex. 5a) {.smaller}

Why limit ourselves to using exclamation points???

```{r}
spam_fit <- logistic_reg() |>
  fit(spam ~ time + exclaim_mess + line_breaks, data = email)
```

## Let's use even more data (Ex. 5b) {.smaller}

What about all the predictors???

```{r}
spam_all_fit <- logistic_reg() |>
  fit(spam ~ ., data = email)
```

The `.` means *all other variables* .

## Is our model good??? (Ex. 5c) {.smaller}

-   Goal is to evaluate how good our model is.
-   If you're coding along: pause!! We're going to review concepts first!!

## Reminder: Classification Error {.smaller}

There are two kinds of mistakes:

![](images/18/confusion-matrix.png)

We want to avoid both, but there's a trade-off.

## Terms: False negative and positive rates {.smaller}

-   **False negative rate** is the *proportion* of actual positives that were classified as negatives.

-   **False positive rate** is the *proportion* of actual negatives that were classified as positives.

. . .

::: callout-tip
We want these to be low!
:::

## Terms: False negative and positive rates {.smaller}

-   **False negative rate** = $\frac{FN}{FN + TP}$

-   **False positive rate** = $\frac{FP}{FP + TN}$

![](images/18/confusion-matrix.png)

## Term: Sensitivity {.smaller}

**Sensitivity** is the *proportion* of actual positives that were correctly classified as positive.

-   Also known as **true positive rate** and **recall**

-   Sensitivity = $\frac{FN}{FN + TP}$

-   Sensitivity = 1 − False negative rate

-   Useful when false negatives are more "expensive" than false positives

::: callout-tip
We want this to be high!
:::

## Term: Specificity {.smaller}

**Specificity** is the *proportion* of actual negatives that were correctly classified as negative

-   Also known as **true negative rate**

-   Specificity = $\frac{TN}{FP + TN}$

-   Specificity = 1 − False positive rate

-   Useful when false positives are more "expensive" than false negatives

::: callout-tip
We want this to be high!
:::

## The augment function {.smaller}

The `augment` function takes a data frame and "augments" it by adding three new columns on the left that describe the model predictions for each row:

::: incremental
-   `.pred_class`: model prediction ($\hat{y}$) based on a 50% threshold;
-   `.pred_0`: model estimate of $P(y=0)$;
-   `.pred_1`: model estimate of $P(y=1) = 1 - P(y = 0)$.
:::

## The augment function {.smaller}

The `augment` function takes a data frame and "augments" it by adding three new columns on the left that describe the model predictions for each row:

```{r}
spam_aug_all <- augment(spam_all_fit, email)
spam_aug_all
```

## Calculating the error rates (5c) {.smaller}

```{r}
spam_aug_all |>
  count(spam, .pred_class) 
```

## Calculating the error rates (5c) {.smaller}

```{r}
spam_aug_all |>
  count(spam, .pred_class) |>
  group_by(spam)
```

## Calculating the error rates (5c) {.smaller}

```{r}
spam_aug_all |>
  count(spam, .pred_class) |>
  group_by(spam) |>
  mutate(p = n / sum(n))
```

## Calculating the error rates (5c) {.smaller}

```{r}
spam_aug_all |>
  count(spam, .pred_class) |>
  group_by(spam) |>
  mutate(p = n / sum(n),
         decision = case_when(
            spam == "0" & .pred_class == "0" ~ "True negative",
            spam == "0" & .pred_class == "1" ~ "False positive",
            spam == "1" & .pred_class == "0" ~ "False negative",
            spam == "1" & .pred_class == "1" ~ "True positive"
        ))
```

This is where we have AE14 stop....

## But wait! {.smaller}

Augment uses a default 50% threshold.

. . .

If we change the classification threshold, we change the classifications, and we change the error rates:

. . .

```{r}
#| code-line-numbers: 2-4
spam_aug_all |>
  mutate(
    .pred_class = if_else(.pred_1 <= 0.25, 0, 1)
  ) |>
  count(spam, .pred_class) |>
  group_by(spam) |>
  mutate(p = n / sum(n))
```

## Classification threshold: 0.00 {.smaller}

```{r}
#| code-line-numbers: 2-4
spam_aug_all |>
  mutate(
    .pred_class = if_else(.pred_1 <= 0.00, 0, 1)
  ) |>
  count(spam, .pred_class) |>
  group_by(spam) |>
  mutate(p = n / sum(n))
```

## Classification threshold: 0.25 {.smaller}

```{r}
#| code-line-numbers: 2-4
spam_aug_all |>
  mutate(
    .pred_class = if_else(.pred_1 <= 0.25, 0, 1)
  ) |>
  count(spam, .pred_class) |>
  group_by(spam) |>
  mutate(p = n / sum(n))
```

## Classification threshold: 0.5 {.smaller}

```{r}
#| code-line-numbers: 2-4
spam_aug_all |>
  mutate(
    .pred_class = if_else(.pred_1 <= 0.50, 0, 1)
  ) |>
  count(spam, .pred_class) |>
  group_by(spam) |>
  mutate(p = n / sum(n))
```

## Classification threshold: 0.75 {.smaller}

```{r}
#| code-line-numbers: 2-4
spam_aug_all |>
  mutate(
    .pred_class = if_else(.pred_1 <= 0.75, 0, 1)
  ) |>
  count(spam, .pred_class) |>
  group_by(spam) |>
  mutate(p = n / sum(n))
```

## Classification threshold: 1.00 {.smaller}

```{r}
#| code-line-numbers: 2-4
spam_aug_all |>
  mutate(
    .pred_class = if_else(.pred_1 <= 1.00, 0, 1)
  ) |>
  count(spam, .pred_class) |>
  group_by(spam) |>
  mutate(p = n / sum(n))
```

## Let's plot these error rates

```{r}
#| echo: false
err_rates <- tibble(
  th = c(0, 0.25, 0.5, 0.75, 1.0),
  `1 - specificity` = c(1.0, 0.08, 0.0092, 0.0028, 0.0),
  sensitivity = c(1.0, 0.531, 0.185, 0.076, 0.0)
)
ggplot(err_rates, aes(x = `1 - specificity`, y = sensitivity, color = "red")) + 
  geom_point() +
  geom_abline(lty = 3) +
    coord_equal() + 
  labs(
    x = "False positive rate = 1 - specificity",
    y = "True positive rate (sensitivity)"
  ) + 
  theme(legend.position="none")
```

## ROC curve {.smaller}

If we repeat this process for "all" possible thresholds $0\leq p^\star\leq 1$, we trace out the **receiver operating characteristic curve** (ROC curve), which assesses the model's performance across a range of thresholds:

```{r}
#| echo: false
roc_curve(spam_aug_all, truth = spam, .pred_1, event_level = "second") |>
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() + 
  labs(
    x = "False positive rate = 1 - specificity",
    y = "True positive rate (sensitivity)"
  )
```

## ROC curve {.smaller}

::: task
Which corner of the plot indicates the best model performance?
:::

![](images/20/roc-curve-annotated.png) 

. . .

Upper left!

## Model comparison

The farther up and to the left the ROC curve is, the better the classification accuracy.
You can quantify this with the area under the curve.

::: callout-note
Area under the ROC curve will be our "quality score" for comparing logistic regression models.
:::

## ROC for full model {.smaller}

```{r}
#| echo: false
roc_curve(spam_aug_all, truth = spam, .pred_1, event_level = "second") |>
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() + 
  labs(
    x = "False positive rate = 1 - specificity",
    y = "True positive rate (sensitivity)"
  )
```

## ROC for simple model {.smaller}

```{r}
#| echo: false
augment(spam_exl_fit, email) |>
  roc_curve(truth = spam, .pred_1, event_level = "second") |>
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() + 
  labs(
    x = "False positive rate = 1 - specificity",
    y = "True positive rate (sensitivity)"
  )
```

## Should we include a predictor? {.smaller}

To determine whether we should include a predictor in a model, we should start by asking:

::: incremental
-   Is it ethical to use this variable?
    (Or even legal?)

-   Will this variable be available at prediction time?

-   Does this variable contribute to explainability?
:::

# Data splitting and spending

## We've been cheating! {.smaller}

::: incremental
-   So far, we've been using all the data we have for building models.
    In predictive contexts, this would be considered *cheating*.

-   Evaluating model performance for predicting outcomes that were used when building the models is like evaluating your learning with questions whose answers you've already seen.
:::

## Spending your data {.smaller}

For predictive models (used primarily in machine learning), we typically split data into training and test sets:

![](images/20/test-train-split-1.svg){fig-align="center"} - The **training set** is used for EDA (plots, summary statistics, etc.)

-   The **training set** is used to estimate model parameters (fit models).

-   The **test set** is used to find an independent assessment of model performance.
    NEVER look at the test set during training.

## How much to spend? {.smaller}

::: incremental
-   The more data we spend (use in training), the better estimates we'll get.

-   Spending too much data in training prevents us from computing a good assessment of predictive performance.

-   Spending too much data in testing prevents us from computing a good estimate of model parameters.
:::

## The initial split {.smaller}

Randomly pick training and testing rows!

```{r}
#| label: initial-split
set.seed(20241112)
email_split <- initial_split(email)
email_split
```

. . .

Default: 75% training; 25% testing

## The initial split {.smaller}

Change proportion:

```{r}
#| label: initial-split-2
set.seed(20250612)
email_split <- initial_split(email, prop = 0.8)
email_split
```

## Setting a seed {.smaller}

::: task
What does `set.seed()` do?
:::

::: incremental
-   To create that split of the data, R generates "pseudo-random" numbers: while they are made to behave like random numbers, their generation is deterministic given a "seed".

-   This allows us to reproduce results by setting that seed.

-   Which seed you pick doesn't matter, as long as you don't try a bunch of seeds and pick the one that gives you the best performance.
:::

## Accessing the data {.smaller}

```{r}
#| label: access-data
email_train <- training(email_split)
email_test <- testing(email_split)
```

## The training set {.smaller}

```{r}
email_train
```

## The testing data {.smaller}

```{r}
email_test
```

## AE 15

-   Create a train and test split
-   Fit models
-   Test and evaluate models

# Washington forests

## Data {.smaller}

-   The U.S. Forest Service maintains machine learning models to predict whether a plot of land is "forested."

-   This classification is important for research, legislation, land management, etc. purposes.

-   Plots are typically remeasured every 10 years.

-   The `forested` dataset contains the most recent measurement per plot.

## Data: `forested` {.smaller}

```{r}
forested
```

## Data: `forested` {.smaller}

```{r}
glimpse(forested)
```

## Outcome and predictors {.smaller}

-   Outcome: `forested` - Factor, `Yes` or `No`

```{r}
levels(forested$forested)
```

-   Predictors: 18 remotely-sensed and easily-accessible predictors:

    -   numeric variables based on weather and topography

    -   categorical variables based on classifications from other governmental organizations

## `?forested`

<iframe width="900" height="500" src="https://simonpcouch.github.io/forested/reference/forested.html" title="forested" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>

</iframe>
