---
title: "Linear Regression"
subtitle: "Lecture 14"
date: "2025-06-05"
format: 
  revealjs:
    output-file: 14-simple-reg-slides.html
    chalkboard: true
---

```{r}
#| include: false
library(tidyverse)
library(tidymodels)
library(fivethirtyeight)
library(datasauRus)
movie_scores <- fandango |>
  rename(
    critics = rottentomatoes, 
    audience = rottentomatoes_user
  )
todays_ae <- "ae-12-modeling-penguins"
```

## While you wait... {.smaller}

::: appex
-   Go to your `ae` project in RStudio.

-   Make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.

-   Click Pull to get today's application exercise file: *`{r} paste0(todays_ae, ".qmd")`*.

-   Wait till the you're prompted to work on the application exercise during class before editing the file.
:::

# Correlation vs. causation

## Spurious correlations

```{r}
#| echo: false
#| message: false

# Create a tibble with year, mozzarella consumption, and engineering degrees
df <- tibble(
  year = 2012:2021,
  mozzarella_cheese_consumption_pounds = c(
    10.6908, 10.7377, 11.1739, 11.2787, 11.7387,
    11.6085, 12.2093, 12.4768, 12.1951, 12.2764
  ),
  bachelors_degrees_awarded_in_engineering = c(
    81371, 85987, 92169, 97852, 106789,
    115671, 121953, 126692, 128337, 126037
  )
)

# Inspect the resulting tibble
df |> ggplot(aes(x = mozzarella_cheese_consumption_pounds,
                 y = bachelors_degrees_awarded_in_engineering)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "lightblue") +
  scale_y_continuous(
    labels = label_number(scale = 1/1000, suffix = "K")
  ) +
  theme_minimal() +
  labs(x = "Mozzarella Cheese Consumption (lbs)", 
       y = "Bachelors Degree Awarded in Chemical Engineering", 
       title = "Mozzarrella vs. Chemical Engineers; r = 0.974", 
       subtitle = "2012-2021", 
       caption = "USDA, National Center for Education Statistics")

```

## Spurious correlations

![](images/15/mozarella-ce-phd.png){fig-align="center"}

::: aside
Source: [tylervigen.com/spurious-correlations](https://www.tylervigen.com/spurious-correlations)
:::

# Linear regression with a single predictor

## Data overview {.smaller}

```{r data-prep}
#| echo: false
movie_scores <- fandango |>
  rename(
    critics = rottentomatoes, 
    audience = rottentomatoes_user
  )
```

```{r data-overview}
#| echo: true
movie_scores |>
  select(critics, audience)
```

## Data visualization {.smaller}

```{r}
#| echo: false
#| fig-height: 4.5
ggplot(movie_scores, aes(x = critics, y = audience)) +
  geom_point(alpha = 0.5) + 
  labs(
    x = "Critics Score" , 
    y = "Audience Score"
  )
```

## Data visualization: linear model {.smaller}

```{r}
#| echo: false
#| message: false
#| fig-height: 4.5
ggplot(movie_scores, aes(x = critics, y = audience)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(alpha = 0.5) + 
  labs(
    x = "Critics Score" , 
    y = "Audience Score"
  )
```

## Data visualization: linear model {.smaller}

```{r}
#| echo: false
#| message: false
#| fig-height: 4.5
ggplot(movie_scores, aes(x = critics, y = audience)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(alpha = 0.5) + 
  labs(
    x = "Critics Score" , 
    y = "Audience Score"
  )
```

::: columns
::: {.column width="60%"}
```{r}
#| echo: false
#| message: false

tidy(lm(audience ~ critics, data = movie_scores))
```
:::

::: {.column width="40%"}
:::
:::

## Data visualization: linear model {.smaller}

```{r}
#| echo: false
#| message: false
#| fig-height: 4.5
ggplot(movie_scores, aes(x = critics, y = audience)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(alpha = 0.5) + 
  labs(
    x = "Critics Score" , 
    y = "Audience Score"
  )
```

::: columns
::: {.column width="60%"}
```{r}
#| echo: false
#| message: false

tidy(lm(audience ~ critics, data = movie_scores))
```
:::

::: {.column width="40%"}
```{r}
#| message: false
#| echo: false

movie_scores |>
  summarize(r = cor(audience, critics))

```
:::
:::

## Prediction: linear model {.smaller}

```{r}
#| message: false
#| echo: false

fit_m = linear_reg() |> fit(audience ~ critics, data = movie_scores)
movie_scores_augment = augment(fit_m, new_data = movie_scores)

movie_scores_augment |> select(critics, audience, .pred, .resid)

```

# Linear Regression: How R Did It

## Regression model

::: fragment
$$
Y \;=\; \mathbf{Model} \;+\; \text{Error}
$$
:::

::: fragment
$$
=\; \mathbf{f(X)} \;+\; \epsilon
$$
:::

::: fragment
$$
=\; \mu_{Y\mid X} \;+\; \epsilon
$$
:::

## Regression model

::: columns
::: {.column width="30%"}
$$
\begin{aligned} Y &= \color{#325b74}{\textbf{Model}} + \text{Error} \\[8pt]
&= \color{#325b74}{\mathbf{f(X)}} + \epsilon \\[8pt]
&= \color{#325b74}{\boldsymbol{\mu_{Y|X}}} + \epsilon 
\end{aligned}
$$
:::

::: {.column width="70%"}
```{r}
#| echo: false
#| message: false

m <- lm(audience ~ critics, data = movie_scores)
ggplot(data = movie_scores, 
       mapping = aes(x = critics, y = audience)) +
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "lm", color = "#325b74", se = FALSE, linewidth = 1.5) +
  labs(x = "X", y = "Y") +
  theme_minimal() +
  theme(
    axis.text = element_blank(),
    axis.ticks.x = element_blank(), 
    axis.ticks.y = element_blank()
    )
```
:::
:::

## Simple linear regression {.smaller}

Use **simple linear regression** to model the relationship between a [quantitative outcome ($Y$)]{style="background-color: #fcefb3"} and a [single quantitative predictor ($X$)]{style="background-color: #fcefb3"}.

. . .

$$\Large{Y = \beta_0 + \beta_1 X + \epsilon}$$

::: incremental
-   $Y$ : True $Y$ values
-   $\beta_1$: True slope of the relationship between $X$ and $Y$
-   $\beta_0$: True intercept of the relationship between $X$ and $Y$
-   $\epsilon$: Error (random noise)
:::

## Simple linear regression {.smaller}

Use **simple linear regression** to model the relationship between a [quantitative outcome ($Y$)]{style="background-color: #fcefb3"} and a [single quantitative predictor ($X$)]{style="background-color: #fcefb3"}.

. . .

$$\Large{\hat{Y} = b_0 + b_1 X}$$

::: incremental
-   $\hat{Y}$ : Fitted $Y$ values
-   $b_1$: Estimated slope of the relationship between $X$ and $Y$
-   $b_0$: Estimated intercept of the relationship between $X$ and $Y$
-   No error term!
:::

## Choosing values for $b_1$ and $b_0$

```{r}
#| echo: false
#| message: false
ggplot(movie_scores, aes(x = critics, y = audience)) +
  geom_point(alpha = 0.4) + 
  geom_abline(intercept = 32.3155, slope = 0.5187, color = "#325b74", linewidth = 1.5) +
  geom_abline(intercept = 25, slope = 0.7, color = "gray") +
  geom_abline(intercept = 21, slope = 0.9, color = "gray") +
  geom_abline(intercept = 35, slope = 0.3, color = "gray") +
  labs(x = "Critics Score", y = "Audience Score")
```

## Residuals {.smaller}

```{r}
#| message: false
#| echo: false
#| fig-align: center
ggplot(movie_scores, aes(x = critics, y = audience)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", color = "#325b74", se = FALSE, linewidth = 1.5) +
  geom_segment(aes(x = critics, xend = critics, y = audience, yend = predict(m)), color = "steel blue") +
  labs(x = "Critics Score", y = "Audience Score") +
  theme(legend.position = "none")
```

$$\small{\text{residual} = \text{observed} - \text{predicted} = y - \hat{y}}$$

## Notation {.smaller}

::: incremental
-   We have $n$ observations (generally, the number of rows in a df)

-   $i^{th}$ observation ($i$ from $1$ to $N$):

    -   $y_i$ : $i^{th}$ outcome

    -   $x_i$ : $i^{th}$ explanatory variable

    -   $\hat{y}$ : $i^{th}$ predicted outcome

    -   $e$ : $i^{th}$ residual
:::

## Notation: Example {.smaller}

*Back to the movies*: audience scores predicted by critic scores.

::: columns
::: {.column width="60%"}
```{r}
#| echo: false
movie_scores_augment |> select(critics, audience, .pred, .resid)
```
:::

::: {.column width="40%"}
::: incremental
-   $x_1 = 74$

-   $y_1 = 86$

-   $\hat{{y}}_1 \approx 71$

-   $e_1 \approx 15$
:::
:::
:::

## Least squares line {.smaller}

-   The residual for the $i^{th}$ observation is

$$e_i = \text{observed} - \text{predicted} = y_i - \hat{y}_i$$

. . .

-   The **sum of squared** residuals is

$$e^2_1 + e^2_2 + \dots + e^2_n$$

. . .

-   The **least squares line** is the one that **minimizes the sum of squared residuals**

## Least squares line {.smaller}

```{r}
#| echo: false
#| message: false
ggplot(movie_scores, aes(x = critics, y = audience)) +
  geom_point(alpha = 0.4) + 
  geom_abline(intercept = 32.3155, slope = 0.5187, color = "#325b74", linewidth = 1.5) +
  geom_abline(intercept = 25, slope = 0.7, color = "gray") +
  geom_abline(intercept = 21, slope = 0.9, color = "gray") +
  geom_abline(intercept = 35, slope = 0.3, color = "gray") +
  labs(x = "Critics Score", y = "Audience Score")
```

## Least squares line {.smaller}

```{r}
movies_fit <- linear_reg() |>
  fit(audience ~ critics, data = movie_scores)

tidy(movies_fit)
```

. . .

Let's interpret this!

# Slope and Intercept

## Interpreting slope & intercept {.smaller}

$$\widehat{y} = b_0 + b_1 \times x$$

::: incremental
-   **Slope:** For every one increase in the value of $x$, we expect $y$ to be higher by $b_1$, on average.

-   **Intercept:** If $x = 0$, we expect $y = b_0$
:::

## Interpreting slope & intercept {.smaller}

$$\widehat{\text{audience}} = 32.3 + 0.519 \times \text{critics}$$

::: incremental
-   **Slope:** For every one point increase in the critics score, we expect the audience score to be higher by 0.519 points, on average.
-   **Intercept:** If the critics score is 0 points, we expect the audience score to be 32.3 points.
:::

## Is the intercept meaningful?

âœ… The intercept is meaningful in context of the data if

-   the predictor can feasibly take values equal to or near zero or
-   the predictor has values near zero in the observed data

. . .

ðŸ›‘ Otherwise, it might not be meaningful!

## Properties of least squares regression {.smaller}

::: incremental
-   Slope ($b_1$) has the same sign as the correlation coefficient ($r$): $b_1 = r \frac{s_Y}{s_X}$

-   The regression line goes through the center of mass point (the coordinates corresponding to average $X$ and average $Y$)

-   Sum of the residuals is zero
:::

# Application exercise

## `{r} todays_ae` {.smaller}

::: appex
-   Go to your ae project in RStudio.

-   If you haven't yet done so, make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.

-   If you haven't yet done so, click Pull to get today's application exercise file: *`{r} paste0(todays_ae, ".qmd")`*.

-   Work through the application exercise in class, and render, commit, and push your edits.
:::

# Regression with Categorical Variables

## Regression with Categorical Variables

What does $b_0 +b_1*x$ even mean when $x$ is categorical ???

. . .

Suppose variable *island* can take values *A, B,* or *C.* We want to model variable *mass* based on *island*.

. . .

We tell R to fit `mass ~ island` ... now what?

. . .

[We get ***dummy variables!!!***]{style="background-color: #fcefb3"}

## Dummy Variables

::: columns
::: {.column width="50%"}
| mass | island |
|------|--------|
| 10   | A      |
| 30   | C      |
| 20   | B      |
| 15   | A      |
:::
:::

## Dummy Variables

::: columns
::: {.column width="50%"}
| mass | island |
|------|--------|
| 10   | A      |
| 30   | C      |
| 20   | B      |
| 15   | A      |
:::

::: {.column width="50%"}
| mass | island | **A** | B   | C   |
|------|--------|-------|-----|-----|
| 10   | A      |       |     |     |
| 30   | C      |       |     |     |
| 20   | B      |       |     |     |
| 15   | A      |       |     |     |
:::
:::

. . .

::: callout-important
In a given row, only *one* of the dummy variables for a given categorical variable can equal 1.
:::

## What will a regression output look like?

$$
mass = b_0 + b_2 * B + b_3 *C
$$

. . .

What is the estimated weight of a penguin from island *A?*

. . .

What is the estimated weight of a penguin from island *B?*

. . .

What is the estimated weight of a penguin from island *C?*

. . .

## Let's take a look at the actual penguins data. Back to the AE!
