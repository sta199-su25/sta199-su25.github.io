{
  "hash": "62e6d63833886e7ab5da0a910394c057",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"AE 14: Building a spam filter\"\n---\n\nIn this application exercise, we will\n\n-   Use logistic regression to fit a model for a binary response variable\n-   Fit a logistic regression model in R\n-   Use a logistic regression model for classification\n\nTo illustrate logistic regression, we will build a spam filter from email data.\n\nThe data come from incoming emails in David Diez's (one of the authors of OpenIntro textbooks) Gmail account for the first three months of 2012.\nAll personally identifiable information has been removed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(email)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 3,921\nColumns: 21\n$ spam         <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ to_multiple  <fct> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ cc           <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 2, 0, …\n$ sent_email   <fct> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, …\n$ time         <dttm> 2012-01-01 00:16:41, 2012-01-01 01:03:59, 2012-01-01 10:…\n$ image        <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ attach       <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dollar       <dbl> 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, …\n$ winner       <fct> no, no, no, no, no, no, no, no, no, no, no, no, no, no, n…\n$ inherit      <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ password     <dbl> 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ num_char     <dbl> 11.370, 10.504, 7.773, 13.256, 1.231, 1.091, 4.837, 7.421…\n$ line_breaks  <int> 202, 202, 192, 255, 29, 25, 193, 237, 69, 68, 25, 79, 191…\n$ format       <fct> 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, …\n$ re_subj      <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, …\n$ exclaim_subj <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exclaim_mess <dbl> 0, 1, 6, 48, 1, 1, 1, 18, 1, 0, 2, 1, 0, 10, 4, 10, 20, 0…\n$ number       <fct> big, small, small, small, none, none, big, small, small, …\n```\n\n\n:::\n:::\n\n\nThe variables we'll use in this analysis are\n\n-   `spam`: 1 if the email is spam, 0 otherwise\n-   `exclaim_mess`: The number of exclamation points in the email message\n\n**Goal:** Use the number of exclamation points in an email to predict whether or not it is spam.\n\n# Exercises\n\n## Exercise 1\n\nLet's start with some exploratory analysis:\n\na.  Create an density plot to investigate the relationship between `spam` and `exclaim_mess`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(email, aes(x = exclaim_mess, fill = spam)) + \n  geom_density()\n```\n\n::: {.cell-output-display}\n![](ae-14-github-A_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nb.  Additionally, calculate the mean number of exclamation points for both spam and non-spam emails.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail |>\n  group_by(spam) |>\n  summarize(mean_ep = mean(exclaim_mess))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  spam  mean_ep\n  <fct>   <dbl>\n1 0        6.51\n2 1        7.32\n```\n\n\n:::\n:::\n\n\n## Exericse 2\n\nVisualize a linear model fit for these data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(email, aes(x = exclaim_mess, y = as.numeric(spam) - 1)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"# of exclamation marks in email\",\n    y = \"e-mail type\"\n  ) +\n  scale_y_continuous(breaks = c(0, 1),\n                   labels = c(\"legit (0)\", \"spam (1)\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](ae-14-github-A_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nIs the linear model a good fit for the data?\nWhy or why not?\n\n*Add response here.*\n\n## Exercise 3\n\na.  Fit the logistic regression model using the number of exclamation points to predict the probability an email is spam:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_fit <- logistic_reg() |>\n  fit(spam ~ exclaim_mess, data = email)\n\ntidy(log_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term          estimate std.error statistic p.value\n  <chr>            <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)  -2.27      0.0553     -41.1     0    \n2 exclaim_mess  0.000272  0.000949     0.287   0.774\n```\n\n\n:::\n:::\n\n\nb.  Add your estimates to the fitted equation below\n\n$$\\log\\Big(\\frac{\\hat{p}}{1-\\hat{p}}\\Big) = -2.27 + 0.00027 \\times exclaim\\_mess$$\n\nc. How does the code above differ from previous code we've used to fit regression models? \n\n**Ans**: `linear_reg` is changed to `logistic_reg`. Things are otherwise unchanged.\n\n\n## Exercise 4\n\na.  What is the probability the email is spam if it contains 10 exclamation points? Answer the question using the `predict()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_email <- tibble(\n  exclaim_mess = 10\n  )\n\npredict(log_fit, new_data = new_email, type = \"prob\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  .pred_0 .pred_1\n    <dbl>   <dbl>\n1   0.906  0.0937\n```\n\n\n:::\n:::\n\n\nb.  A probability is nice, but we want an actual decision. Classify the darn email.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(log_fit, new_data = new_email, type = \"class\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  .pred_class\n  <fct>      \n1 0          \n```\n\n\n:::\n:::\n\n\n## Exercise 5\n\na.  Fit a model with three variables of your choosing in the dataset as predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# answers may vary\n```\n:::\n\n\nb.  Fit a model with **all** variables in the dataset as predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_fit2 <- logistic_reg() |>\n  fit(spam ~ ., data = email)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n```\n\n\n:::\n:::\n\n\nc.  If you used this model to classify the emails in the dataset, how would it do? Use the fitted model to classify each email in the dataset, and then calculate the classification error rates (TP, TN, FP, FN).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_aug <- augment(log_fit2, email)\n\n\nlog_aug |>\n  count(spam, .pred_class) |>\n  group_by(spam) |>\n  mutate(p = n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 4\n# Groups:   spam [2]\n  spam  .pred_class     n       p\n  <fct> <fct>       <int>   <dbl>\n1 0     0            3521 0.991  \n2 0     1              33 0.00929\n3 1     0             299 0.815  \n4 1     1              68 0.185  \n```\n\n\n:::\n:::\n\n",
    "supporting": [
      "ae-14-github-A_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}