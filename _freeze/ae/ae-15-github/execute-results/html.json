{
  "hash": "1222b55992f26c9df6cdb02b5a9fe7b5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"AE 15: Forest classification\"\n---\n\nIn this application exercise, we will\n\n-   Split our data into testing and training\n-   Fit logistic regression regression models to testing data to classify outcomes\n-   Evaluate performance of models on testing data\n\nWe will use **tidyverse** and **tidymodels** for data exploration and modeling, respectively, and the **forested** package for the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(forested)\n```\n:::\n\n\nRemember from the lecture that the `forested` dataset contains information on whether a plot is forested (`Yes`) or not (`No`) as well as numerical and categorical features of that plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(forested)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 7,107\nColumns: 19\n$ forested         <fct> Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes,…\n$ year             <dbl> 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005,…\n$ elevation        <dbl> 881, 113, 164, 299, 806, 736, 636, 224, 52, 2240, 104…\n$ eastness         <dbl> 90, -25, -84, 93, 47, -27, -48, -65, -62, -67, 96, -4…\n$ northness        <dbl> 43, 96, 53, 34, -88, -96, 87, -75, 78, -74, -26, 86, …\n$ roughness        <dbl> 63, 30, 13, 6, 35, 53, 3, 9, 42, 99, 51, 190, 95, 212…\n$ tree_no_tree     <fct> Tree, Tree, Tree, No tree, Tree, Tree, No tree, Tree,…\n$ dew_temp         <dbl> 0.04, 6.40, 6.06, 4.43, 1.06, 1.35, 1.42, 6.39, 6.50,…\n$ precip_annual    <dbl> 466, 1710, 1297, 2545, 609, 539, 702, 1195, 1312, 103…\n$ temp_annual_mean <dbl> 6.42, 10.64, 10.07, 9.86, 7.72, 7.89, 7.61, 10.45, 10…\n$ temp_annual_min  <dbl> -8.32, 1.40, 0.19, -1.20, -5.98, -6.00, -5.76, 1.11, …\n$ temp_annual_max  <dbl> 12.91, 15.84, 14.42, 15.78, 13.84, 14.66, 14.23, 15.3…\n$ temp_january_min <dbl> -0.08, 5.44, 5.72, 3.95, 1.60, 1.12, 0.99, 5.54, 6.20…\n$ vapor_min        <dbl> 78, 34, 49, 67, 114, 67, 67, 31, 60, 79, 172, 162, 70…\n$ vapor_max        <dbl> 1194, 938, 754, 1164, 1254, 1331, 1275, 944, 892, 549…\n$ canopy_cover     <dbl> 50, 79, 47, 42, 59, 36, 14, 27, 82, 12, 74, 66, 83, 6…\n$ lon              <dbl> -118.6865, -123.0825, -122.3468, -121.9144, -117.8841…\n$ lat              <dbl> 48.69537, 47.07991, 48.77132, 45.80776, 48.07396, 48.…\n$ land_type        <fct> Tree, Tree, Tree, Tree, Tree, Tree, Non-tree vegetati…\n```\n\n\n:::\n:::\n\n\n# Spending your data\n\nSplit your data into testing and training in a reproducible manner and display the split object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nforested_split <- initial_split(forested)\nforested_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Testing/Total>\n<5330/1777/7107>\n```\n\n\n:::\n:::\n\n\nNow, save your training and testing data as their own data frames. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_train <- training(forested_split)\nforested_test <- testing(forested_split)\n```\n:::\n\n\n# Exploratory data analysis\n\nCreate some visualizations to explore the data! This can help you determine which predictors you want to include in your model.\n\n**Note:** Pay attention to which dataset you use for your exploration.\n\nThis is a plot that explores latitude and longitude - it's different from anything we have seen so far!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(forested_train, aes(x = lon, y = lat, color = forested)) +\n  geom_point(alpha = 0.7) +\n  scale_color_manual(values = c(\"Yes\" = \"forestgreen\", \"No\" = \"gold2\")) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](ae-15-github_files/figure-html/eda-plot-1.png){width=672}\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add some other plots here!\n```\n:::\n\n\n\n# Model 1: Custom choice of predictors\n\n## Fit\n\nFit a model for classifying plots as forested or not based on a subset of predictors of your choice. Name the model `forested_custom_fit` and display a tidy output of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_custom_fit <- logistic_reg() |>\n  fit(_____ ~ ____________, \n      data = __________)\n\ntidy(forested_custom_fit)\n```\n:::\n\n\n## Predict\n\nPredict for the testing data using this model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_custom_aug <- _________(forested_custom_fit, new_data = _______)\n\nforested_custom_aug\n```\n:::\n\n\n## Evaluate\n\nCalculate the false positive and false negative rates for the testing data using this model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_custom_aug |>\n  count(.pred_class, forested) |>\n  arrange(forested) |>\n  group_by(forested) |>\n  mutate(\n    p = round(n / sum(n), 2),\n    decision = case_when(\n      .pred_class == \"Yes\" & forested == \"Yes\" ~ \"True positive\",\n      .pred_class == \"Yes\" & forested == \"No\" ~ \"False positive\",\n      .pred_class == \"No\" & forested == \"Yes\" ~ \"False negative\",\n      .pred_class == \"No\" & forested == \"No\" ~ \"True negative\"\n    )\n  )\n```\n:::\n\n\nAnother commonly used display of this information is a confusion matrix. Create this using the `conf_mat()` function. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_mat(\n  forested_custom_aug, \n  truth = forested, \n  estimate = .pred_class\n)\n```\n:::\n\n\n## Sensitivity, specificity, ROC curve\n\nCalculate sensitivity and specificity and draw the ROC curve.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_custom_roc <- roc_curve(forested_custom_aug, \n                                 truth = ____,  # column with truth\n                                 _____, # column with y = 1 preds\n                                 event_level = _____) # factor level for y = 1\n\nforested_custom_roc\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(forested_custom_roc, aes(x = 1 - ______, y = ______)) +\n  geom_path() + # draws line\n  geom_abline(lty = 3) + # x = y line\n  coord_equal() # makes square\n```\n:::\n\n\n# Model 2: All predictors\n\n## Fit\n\nFit a model for classifying plots as forested or not based on all predictors available. Name the model `forested_full_fit` and display a tidy output of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_full_fit <- logistic_reg() |>\n  fit(_________ ~ ________, data = _________)\n\ntidy(forested_full_fit)\n```\n:::\n\n\n## Predict\n\nPredict for the testing data using this model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_full_aug <- augment(____________, new_data = ________)\n\nforested_full_aug\n```\n:::\n\n\n## Evaluate\n\nCalculate the false positive and false negative rates for the testing data using this model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_full_aug |>\n  count(.pred_class, forested) |>\n  arrange(forested) |>\n  group_by(forested) |>\n  mutate(\n    p = round(n / sum(n), 2),\n    decision = case_when(\n      .pred_class == \"Yes\" & forested == \"Yes\" ~ \"True positive\",\n      .pred_class == \"Yes\" & forested == \"No\" ~ \"False positive\",\n      .pred_class == \"No\" & forested == \"Yes\" ~ \"False negative\",\n      .pred_class == \"No\" & forested == \"No\" ~ \"True negative\"\n    )\n  )\n```\n:::\n\n\n## Sensitivity, specificity, ROC curve\n\nCalculate sensitivity and specificity and draw the ROC curve.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_full_roc <- roc_curve(forested_full_aug, \n                               truth = ____,  # column with truth\n                                 _____, # column with y = 1 preds\n                                 event_level = _____ # factor level for y = 1\n                               ) \n\nforested_full_roc\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(forested_full_roc, aes(x = 1 - _______, y = ______)) +\n  geom_path() + # draws line\n  geom_abline(lty = 3) + # x = y line\n  coord_equal() #makes plot square\n```\n:::\n\n\n# Model 1 vs. Model 2\n\nlot both ROC curves and articulate how you would use them to compare these models.\n\nFirst, add a column to each roc data.\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_custom_roc <- forested_custom_roc |>\n  mutate(model = \"Custom\")\n\nforested_full_roc <- forested_full_roc |>\n  mutate(model = \"Full\")\n```\n:::\n\n\nNext, combine data.\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_combined <- bind_rows(forested_custom_roc, forested_full_roc) \n```\n:::\n\n\nNow, plot!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n___________ |>\n  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +\n  geom_path() + # draws line\n  geom_abline(lty = 3) + # adds x = y line\n  coord_equal() # makes square\n```\n:::\n\n\nThe full model looks better. We can quantify this comparison with the area under the curve:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# same as roc_curve, but roc_auc\n\nfull_roc_auc <- _______ (\n  forested_full_aug, \n  truth = forested, \n  .pred_Yes, \n  event_level = \"first\"\n)\n\n# same as roc_curve, but roc_auc\ncustom_roc_auc <- _______ (\n  forested_custom_aug, \n  truth = forested, \n  .pred_Yes, \n  event_level = \"first\"\n)\n\nfull_roc_auc\ncustom_roc_auc\n```\n:::\n\n",
    "supporting": [
      "ae-15-github_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}