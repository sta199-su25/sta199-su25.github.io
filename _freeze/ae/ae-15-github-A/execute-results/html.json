{
  "hash": "4ec2a70fada7cbe511bfc29d1e10f148",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"AE 15: Forest classification\"\n---\n\nIn this application exercise, we will\n\n-   Split our data into testing and training\n-   Fit logistic regression regression models to testing data to classify outcomes\n-   Evaluate performance of models on testing data\n\nWe will use **tidyverse** and **tidymodels** for data exploration and modeling, respectively, and the **forested** package for the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(forested)\n\n#install.packages(\"forested\")\n```\n:::\n\n\nRemember from the lecture that the `forested` dataset contains information on whether a plot is forested (`Yes`) or not (`No`) as well as numerical and categorical features of that plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(forested)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 7,107\nColumns: 19\n$ forested         <fct> Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes,…\n$ year             <dbl> 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005,…\n$ elevation        <dbl> 881, 113, 164, 299, 806, 736, 636, 224, 52, 2240, 104…\n$ eastness         <dbl> 90, -25, -84, 93, 47, -27, -48, -65, -62, -67, 96, -4…\n$ northness        <dbl> 43, 96, 53, 34, -88, -96, 87, -75, 78, -74, -26, 86, …\n$ roughness        <dbl> 63, 30, 13, 6, 35, 53, 3, 9, 42, 99, 51, 190, 95, 212…\n$ tree_no_tree     <fct> Tree, Tree, Tree, No tree, Tree, Tree, No tree, Tree,…\n$ dew_temp         <dbl> 0.04, 6.40, 6.06, 4.43, 1.06, 1.35, 1.42, 6.39, 6.50,…\n$ precip_annual    <dbl> 466, 1710, 1297, 2545, 609, 539, 702, 1195, 1312, 103…\n$ temp_annual_mean <dbl> 6.42, 10.64, 10.07, 9.86, 7.72, 7.89, 7.61, 10.45, 10…\n$ temp_annual_min  <dbl> -8.32, 1.40, 0.19, -1.20, -5.98, -6.00, -5.76, 1.11, …\n$ temp_annual_max  <dbl> 12.91, 15.84, 14.42, 15.78, 13.84, 14.66, 14.23, 15.3…\n$ temp_january_min <dbl> -0.08, 5.44, 5.72, 3.95, 1.60, 1.12, 0.99, 5.54, 6.20…\n$ vapor_min        <dbl> 78, 34, 49, 67, 114, 67, 67, 31, 60, 79, 172, 162, 70…\n$ vapor_max        <dbl> 1194, 938, 754, 1164, 1254, 1331, 1275, 944, 892, 549…\n$ canopy_cover     <dbl> 50, 79, 47, 42, 59, 36, 14, 27, 82, 12, 74, 66, 83, 6…\n$ lon              <dbl> -118.6865, -123.0825, -122.3468, -121.9144, -117.8841…\n$ lat              <dbl> 48.69537, 47.07991, 48.77132, 45.80776, 48.07396, 48.…\n$ land_type        <fct> Tree, Tree, Tree, Tree, Tree, Tree, Non-tree vegetati…\n```\n\n\n:::\n:::\n\n\n# Spending your data\n\nSplit your data into testing and training in a reproducible manner and display the split object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nforested_split <- initial_split(forested)\nforested_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Testing/Total>\n<5330/1777/7107>\n```\n\n\n:::\n:::\n\n\nNow, save your training and testing data as their own data frames.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_train <- training(forested_split)\nforested_test <- testing(forested_split)\n```\n:::\n\n\n# Exploratory data analysis\n\nCreate some visualizations to explore the data! This can help you determine which predictors you want to include in your model.\n\n**Note:** Pay attention to which dataset you use for your exploration.\n\nThis is a plot that explores latitude and longitude - it's different from anything we have seen so far!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(forested_train, aes(x = lon, y = lat, color = forested)) +\n  geom_point(alpha = 0.7) +\n  scale_color_manual(values = c(\"Yes\" = \"forestgreen\", \"No\" = \"gold2\")) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](ae-15-github-A_files/figure-html/eda-plot-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add some other plots here!\n\nggplot(forested_train, aes(x = tree_no_tree, fill = forested)) +\n  geom_bar(position = \"fill\")+\n  scale_fill_manual(values = c(\"Yes\" = \"forestgreen\", \"No\" = \"gold2\")) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](ae-15-github-A_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(forested_train, aes(x = temp_annual_mean, color = forested)) +\n  geom_density() +\n  scale_color_manual(values = c(\"Yes\" = \"forestgreen\", \"No\" = \"gold2\")) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](ae-15-github-A_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n\n\n# Model 1: Custom choice of predictors\n\n## Fit\n\nFit a model for classifying plots as forested or not based on a subset of predictors of your choice. Name the model `forested_custom_fit` and display a tidy output of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_custom_fit <- logistic_reg() |>\n  fit(forested ~ lat + lon + tree_no_tree + temp_annual_mean, \n      data = forested_train)\n\ntidy(forested_custom_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 5\n  term                estimate std.error statistic   p.value\n  <chr>                  <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)           44.7      4.13       10.8  2.93e- 27\n2 lat                   -0.285    0.0544     -5.25 1.51e-  7\n3 lon                    0.298    0.0242     12.3  6.63e- 35\n4 tree_no_treeNo tree    3.35     0.0920     36.4  6.65e-290\n5 temp_annual_mean       0.334    0.0219     15.2  1.94e- 52\n```\n\n\n:::\n:::\n\n\n## Predict\n\nPredict for the testing data using this model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_custom_aug <- augment(forested_custom_fit, new_data = forested_test)\n\nforested_custom_aug\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,777 × 22\n   .pred_class .pred_Yes .pred_No forested  year elevation eastness northness\n   <fct>           <dbl>    <dbl> <fct>    <dbl>     <dbl>    <dbl>     <dbl>\n 1 Yes            0.878    0.122  Yes       2005       113      -25        96\n 2 Yes            0.833    0.167  Yes       2005       736      -27       -96\n 3 Yes            0.855    0.145  Yes       2005       224      -65       -75\n 4 Yes            0.955    0.0451 Yes       2003      1031      -49        86\n 5 Yes            0.716    0.284  No        2005      1713      -66        75\n 6 Yes            0.982    0.0181 Yes       2014      1612       30       -95\n 7 No             0.0781   0.922  No        2014       507       44       -89\n 8 Yes            0.885    0.115  Yes       2014       940      -93        35\n 9 Yes            0.653    0.347  No        2014       246       22       -97\n10 No             0.0890   0.911  No        2014       419       86       -49\n# ℹ 1,767 more rows\n# ℹ 14 more variables: roughness <dbl>, tree_no_tree <fct>, dew_temp <dbl>,\n#   precip_annual <dbl>, temp_annual_mean <dbl>, temp_annual_min <dbl>,\n#   temp_annual_max <dbl>, temp_january_min <dbl>, vapor_min <dbl>,\n#   vapor_max <dbl>, canopy_cover <dbl>, lon <dbl>, lat <dbl>, land_type <fct>\n```\n\n\n:::\n:::\n\n\n## Evaluate\n\nCalculate the false positive and false negative rates for the testing data using this model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_custom_aug |>\n  count(.pred_class, forested) |>\n  arrange(forested) |>\n  group_by(forested) |>\n  mutate(\n    p = round(n / sum(n), 2),\n    decision = case_when(\n      .pred_class == \"Yes\" & forested == \"Yes\" ~ \"True positive\",\n      .pred_class == \"Yes\" & forested == \"No\" ~ \"False positive\",\n      .pred_class == \"No\" & forested == \"Yes\" ~ \"False negative\",\n      .pred_class == \"No\" & forested == \"No\" ~ \"True negative\"\n    )\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n# Groups:   forested [2]\n  .pred_class forested     n     p decision      \n  <fct>       <fct>    <int> <dbl> <chr>         \n1 Yes         Yes        864  0.9  True positive \n2 No          Yes         98  0.1  False negative\n3 Yes         No         123  0.15 False positive\n4 No          No         692  0.85 True negative \n```\n\n\n:::\n:::\n\n\nAnother commonly used display of this information is a confusion matrix. Create this using the `conf_mat()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_mat(\n  forested_custom_aug, \n  truth = forested, \n  estimate = .pred_class\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction Yes  No\n       Yes 864 123\n       No   98 692\n```\n\n\n:::\n:::\n\n\n## Sensitivity, specificity, ROC curve\n\nCalculate sensitivity and specificity and draw the ROC curve.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_custom_roc <- roc_curve(forested_custom_aug, \n                                 truth = forested,  # column with truth\n                                 .pred_Yes, # column with y = 1 preds\n                                 event_level = \"first\") # factor level for y = 1\n\nforested_custom_roc\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,779 × 3\n   .threshold specificity sensitivity\n        <dbl>       <dbl>       <dbl>\n 1  -Inf          0                 1\n 2     0.0179     0                 1\n 3     0.0187     0.00123           1\n 4     0.0195     0.00245           1\n 5     0.0199     0.00368           1\n 6     0.0219     0.00491           1\n 7     0.0282     0.00613           1\n 8     0.0287     0.00736           1\n 9     0.0294     0.00859           1\n10     0.0295     0.00982           1\n# ℹ 1,769 more rows\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(forested_custom_roc, aes(x = 1 - specificity, y = sensitivity)) +\n  geom_path() + # draws line\n  geom_abline(lty = 3) + # x = y line\n  coord_equal() # makes square\n```\n\n::: {.cell-output-display}\n![](ae-15-github-A_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n# Model 2: All predictors\n\n## Fit\n\nFit a model for classifying plots as forested or not based on all predictors available. Name the model `forested_full_fit` and display a tidy output of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_full_fit <- logistic_reg() |>\n  fit(forested ~ ., data = forested_train)\n\ntidy(forested_full_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 × 5\n   term                             estimate std.error statistic  p.value\n   <chr>                               <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)                  -12.1        32.6       -0.371   7.11e- 1\n 2 year                           0.00456     0.0152     0.299   7.65e- 1\n 3 elevation                     -0.00277     0.000639  -4.33    1.47e- 5\n 4 eastness                      -0.000910    0.000734  -1.24    2.15e- 1\n 5 northness                      0.00208     0.000745   2.79    5.26e- 3\n 6 roughness                     -0.00399     0.00146   -2.73    6.29e- 3\n 7 tree_no_treeNo tree            1.25        0.136      9.23    2.61e-20\n 8 dew_temp                      -0.125       0.176     -0.712   4.76e- 1\n 9 precip_annual                 -0.0000895   0.000100  -0.895   3.71e- 1\n10 temp_annual_mean              -7.30       12.4       -0.587   5.57e- 1\n11 temp_annual_min                0.819       0.103      7.93    2.20e-15\n12 temp_annual_max                2.59        6.22       0.417   6.77e- 1\n13 temp_january_min               3.34        6.21       0.538   5.91e- 1\n14 vapor_min                      0.00000990  0.00353    0.00280 9.98e- 1\n15 vapor_max                      0.00925     0.00132    7.00    2.62e-12\n16 canopy_cover                  -0.0446      0.00366  -12.2     4.18e-34\n17 lon                           -0.0953      0.0559    -1.71    8.80e- 2\n18 lat                            0.0748      0.109      0.683   4.94e- 1\n19 land_typeNon-tree vegetation  -0.735       0.282     -2.61    9.05e- 3\n20 land_typeTree                 -1.58        0.297     -5.33    9.93e- 8\n```\n\n\n:::\n:::\n\n\n## Predict\n\nPredict for the testing data using this model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_full_aug <- augment(forested_full_fit, new_data = forested_test)\n\nforested_full_aug\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,777 × 22\n   .pred_class .pred_Yes .pred_No forested  year elevation eastness northness\n   <fct>           <dbl>    <dbl> <fct>    <dbl>     <dbl>    <dbl>     <dbl>\n 1 Yes            0.930    0.0700 Yes       2005       113      -25        96\n 2 Yes            0.918    0.0822 Yes       2005       736      -27       -96\n 3 No             0.428    0.572  Yes       2005       224      -65       -75\n 4 Yes            0.972    0.0280 Yes       2003      1031      -49        86\n 5 Yes            0.633    0.367  No        2005      1713      -66        75\n 6 Yes            0.980    0.0201 Yes       2014      1612       30       -95\n 7 No             0.0436   0.956  No        2014       507       44       -89\n 8 Yes            0.845    0.155  Yes       2014       940      -93        35\n 9 No             0.0141   0.986  No        2014       246       22       -97\n10 No             0.0386   0.961  No        2014       419       86       -49\n# ℹ 1,767 more rows\n# ℹ 14 more variables: roughness <dbl>, tree_no_tree <fct>, dew_temp <dbl>,\n#   precip_annual <dbl>, temp_annual_mean <dbl>, temp_annual_min <dbl>,\n#   temp_annual_max <dbl>, temp_january_min <dbl>, vapor_min <dbl>,\n#   vapor_max <dbl>, canopy_cover <dbl>, lon <dbl>, lat <dbl>, land_type <fct>\n```\n\n\n:::\n:::\n\n\n## Evaluate\n\nCalculate the false positive and false negative rates for the testing data using this model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_full_aug |>\n  count(.pred_class, forested) |>\n  arrange(forested) |>\n  group_by(forested) |>\n  mutate(\n    p = round(n / sum(n), 2),\n    decision = case_when(\n      .pred_class == \"Yes\" & forested == \"Yes\" ~ \"True positive\",\n      .pred_class == \"Yes\" & forested == \"No\" ~ \"False positive\",\n      .pred_class == \"No\" & forested == \"Yes\" ~ \"False negative\",\n      .pred_class == \"No\" & forested == \"No\" ~ \"True negative\"\n    )\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n# Groups:   forested [2]\n  .pred_class forested     n     p decision      \n  <fct>       <fct>    <int> <dbl> <chr>         \n1 Yes         Yes        876  0.91 True positive \n2 No          Yes         86  0.09 False negative\n3 Yes         No          85  0.1  False positive\n4 No          No         730  0.9  True negative \n```\n\n\n:::\n:::\n\n\n## Sensitivity, specificity, ROC curve\n\nCalculate sensitivity and specificity and draw the ROC curve.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_full_roc <- roc_curve(forested_full_aug, \n                               truth = forested,  # column with truth\n                                 .pred_Yes, # column with y = 1 preds\n                                 event_level = \"first\" # factor level for y = 1\n                               ) \n\nforested_full_roc\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,779 × 3\n   .threshold specificity sensitivity\n        <dbl>       <dbl>       <dbl>\n 1 -Inf           0                 1\n 2    0.00106     0                 1\n 3    0.00107     0.00123           1\n 4    0.00217     0.00245           1\n 5    0.00287     0.00368           1\n 6    0.00290     0.00491           1\n 7    0.00290     0.00613           1\n 8    0.00309     0.00736           1\n 9    0.00321     0.00859           1\n10    0.00323     0.00982           1\n# ℹ 1,769 more rows\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(forested_full_roc, aes(x = 1 - specificity, y = sensitivity)) +\n  geom_path() + # draws line\n  geom_abline(lty = 3) + # x = y line\n  coord_equal() #makes plot square\n```\n\n::: {.cell-output-display}\n![](ae-15-github-A_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n# Model 1 vs. Model 2\n\nPlot both ROC curves and articulate how you would use them to compare these models.\n\nFirst, add a column to each roc data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_custom_roc <- forested_custom_roc |>\n  mutate(model = \"Custom\")\n\nforested_full_roc <- forested_full_roc |>\n  mutate(model = \"Full\")\n```\n:::\n\n\nNext, combine data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_combined <- bind_rows(forested_custom_roc, forested_full_roc) \n```\n:::\n\n\nNow, plot!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_combined |>\n  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +\n  geom_path() + # draws line\n  geom_abline(lty = 3) + # adds x = y line\n  coord_equal() # makes square\n```\n\n::: {.cell-output-display}\n![](ae-15-github-A_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nThe full model looks better. We can quantify this comparison with the area under the curve:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# same as roc_curve, but roc_auc\n\nfull_roc_auc <- roc_auc (\n  forested_full_aug, \n  truth = forested, \n  .pred_Yes, \n  event_level = \"first\"\n)\n\n# same as roc_curve, but roc_auc\ncustom_roc_auc <- roc_auc (\n  forested_custom_aug, \n  truth = forested, \n  .pred_Yes, \n  event_level = \"first\"\n)\n\nfull_roc_auc\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.965\n```\n\n\n:::\n\n```{.r .cell-code}\ncustom_roc_auc\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.943\n```\n\n\n:::\n:::\n",
    "supporting": [
      "ae-15-github-A_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}