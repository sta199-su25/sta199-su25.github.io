{
  "hash": "4463f5b5817f8b622e5ec3bdf9500ed2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Regression\"\nsubtitle: \"Lecture 14\"\ndate: \"2025-06-05\"\nformat: \n  revealjs:\n    output-file: 14-simple-reg-slides.html\n    chalkboard: \n      theme: whiteboard\n      chalk-effect: 0\n      chalk-width: 6\n---\n\n\n\n## While you wait... {.smaller}\n\n::: appex\n-   Go to your `ae` project in RStudio.\n\n-   Make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.\n\n-   Click Pull to get today's application exercise file: *ae\\-12\\-modeling\\-penguins\\.qmd*.\n\n-   Wait till the you're prompted to work on the application exercise during class before editing the file.\n:::\n\n# Correlation vs. causation\n\n## Spurious correlations\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-simple-reg_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Spurious correlations\n\n![](images/15/mozarella-ce-phd.png){fig-align=\"center\"}\n\n::: aside\nSource: [tylervigen.com/spurious-correlations](https://www.tylervigen.com/spurious-correlations)\n:::\n\n# Linear regression with a single predictor\n\n## Data overview {.smaller}\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmovie_scores |>\n  select(critics, audience)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 146 Ã— 2\n   critics audience\n     <int>    <int>\n 1      74       86\n 2      85       80\n 3      80       90\n 4      18       84\n 5      14       28\n 6      63       62\n 7      42       53\n 8      86       64\n 9      99       82\n10      89       87\n# â„¹ 136 more rows\n```\n\n\n:::\n:::\n\n\n## Data visualization {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-simple-reg_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## Data visualization: linear model {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-simple-reg_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## Data visualization: linear model {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-simple-reg_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n::: columns\n::: {.column width=\"60%\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   32.3      2.34        13.8 4.03e-28\n2 critics        0.519    0.0345      15.0 2.70e-31\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n:::\n:::\n\n## Data visualization: linear model {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-simple-reg_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n::: columns\n::: {.column width=\"60%\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   32.3      2.34        13.8 4.03e-28\n2 critics        0.519    0.0345      15.0 2.70e-31\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 1\n      r\n  <dbl>\n1 0.781\n```\n\n\n:::\n:::\n\n:::\n:::\n\n## Prediction: linear model {.smaller}\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 146 Ã— 4\n   critics audience .pred .resid\n     <int>    <int> <dbl>  <dbl>\n 1      74       86  70.7  15.3 \n 2      85       80  76.4   3.60\n 3      80       90  73.8  16.2 \n 4      18       84  41.7  42.3 \n 5      14       28  39.6 -11.6 \n 6      63       62  65.0  -2.99\n 7      42       53  54.1  -1.10\n 8      86       64  76.9 -12.9 \n 9      99       82  83.7  -1.66\n10      89       87  78.5   8.52\n# â„¹ 136 more rows\n```\n\n\n:::\n:::\n\n\n# Linear Regression: How R Did It\n\n## Regression model\n\n::: fragment\n$$\nY \\;=\\; \\mathbf{Model} \\;+\\; \\text{Error}\n$$\n:::\n\n::: fragment\n$$\n=\\; \\mathbf{f(X)} \\;+\\; \\epsilon\n$$\n:::\n\n::: fragment\n$$\n=\\; \\mu_{Y\\mid X} \\;+\\; \\epsilon\n$$\n:::\n\n## Regression model\n\n::: columns\n::: {.column width=\"30%\"}\n$$\n\\begin{aligned} Y &= \\color{#325b74}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{#325b74}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{#325b74}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon \n\\end{aligned}\n$$\n:::\n\n::: {.column width=\"70%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-simple-reg_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n:::\n:::\n\n## Simple linear regression {.smaller}\n\nUse **simple linear regression** to model the relationship between a [quantitative outcome ($Y$)]{style=\"background-color: #fcefb3\"} and a [single quantitative predictor ($X$)]{style=\"background-color: #fcefb3\"}.\n\n. . .\n\n$$\\Large{Y = \\beta_0 + \\beta_1 X + \\epsilon}$$\n\n::: incremental\n-   $Y$ : True $Y$ values\n-   $\\beta_1$: True slope of the relationship between $X$ and $Y$\n-   $\\beta_0$: True intercept of the relationship between $X$ and $Y$\n-   $\\epsilon$: Error (random noise)\n:::\n\n## Simple linear regression {.smaller}\n\nUse **simple linear regression** to model the relationship between a [quantitative outcome ($Y$)]{style=\"background-color: #fcefb3\"} and a [single quantitative predictor ($X$)]{style=\"background-color: #fcefb3\"}.\n\n. . .\n\n$$\\Large{\\hat{Y} = b_0 + b_1 X}$$\n\n::: incremental\n-   $\\hat{Y}$ : Fitted $Y$ values\n-   $b_1$: Estimated slope of the relationship between $X$ and $Y$\n-   $b_0$: Estimated intercept of the relationship between $X$ and $Y$\n-   No error term!\n:::\n\n## Choosing values for $b_1$ and $b_0$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-simple-reg_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## Residuals {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](14-simple-reg_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n$$\\small{\\text{residual} = \\text{observed} - \\text{predicted} = y - \\hat{y}}$$\n\n## Notation {.smaller}\n\n::: incremental\n-   We have $n$ observations (generally, the number of rows in a df)\n\n-   $i^{th}$ observation ($i$ from $1$ to $N$):\n\n    -   $y_i$ : $i^{th}$ outcome\n\n    -   $x_i$ : $i^{th}$ explanatory variable\n\n    -   $\\hat{y}$ : $i^{th}$ predicted outcome\n\n    -   $e$ : $i^{th}$ residual\n:::\n\n## Notation: Example {.smaller}\n\n*Back to the movies*: audience scores predicted by critic scores.\n\n::: columns\n::: {.column width=\"60%\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 146 Ã— 4\n   critics audience .pred .resid\n     <int>    <int> <dbl>  <dbl>\n 1      74       86  70.7  15.3 \n 2      85       80  76.4   3.60\n 3      80       90  73.8  16.2 \n 4      18       84  41.7  42.3 \n 5      14       28  39.6 -11.6 \n 6      63       62  65.0  -2.99\n 7      42       53  54.1  -1.10\n 8      86       64  76.9 -12.9 \n 9      99       82  83.7  -1.66\n10      89       87  78.5   8.52\n# â„¹ 136 more rows\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n::: incremental\n-   $x_1 = 74$\n\n-   $y_1 = 86$\n\n-   $\\hat{{y}}_1 \\approx 71$\n\n-   $e_1 \\approx 15$\n:::\n:::\n:::\n\n## Least squares line {.smaller}\n\n-   The residual for the $i^{th}$ observation is\n\n$$e_i = \\text{observed} - \\text{predicted} = y_i - \\hat{y}_i$$\n\n. . .\n\n-   The **sum of squared** residuals is\n\n$$e^2_1 + e^2_2 + \\dots + e^2_n$$\n\n. . .\n\n-   The **least squares line** is the one that **minimizes the sum of squared residuals**\n\n## Least squares line {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-simple-reg_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n## Least squares line {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmovies_fit <- linear_reg() |>\n  fit(audience ~ critics, data = movie_scores)\n\ntidy(movies_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   32.3      2.34        13.8 4.03e-28\n2 critics        0.519    0.0345      15.0 2.70e-31\n```\n\n\n:::\n:::\n\n\n. . .\n\nLet's interpret this!\n\n# Slope and Intercept\n\n## Interpreting slope & intercept {.smaller}\n\n$$\\widehat{y} = b_0 + b_1 \\times x$$\n\n::: incremental\n-   **Slope:** For every one increase in the value of $x$, we expect $y$ to be higher by $b_1$, on average.\n\n-   **Intercept:** If $x = 0$, we expect $y = b_0$\n:::\n\n## Interpreting slope & intercept {.smaller}\n\n$$\\widehat{\\text{audience}} = 32.3 + 0.519 \\times \\text{critics}$$\n\n::: incremental\n-   **Slope:** For every one point increase in the critics score, we expect the audience score to be higher by 0.519 points, on average.\n-   **Intercept:** If the critics score is 0 points, we expect the audience score to be 32.3 points.\n:::\n\n## Is the intercept meaningful?\n\nâœ… The intercept is meaningful in context of the data if\n\n-   the predictor can feasibly take values equal to or near zero or\n-   the predictor has values near zero in the observed data\n\n. . .\n\nðŸ›‘ Otherwise, it might not be meaningful!\n\n## Properties of least squares regression {.smaller}\n\n::: incremental\n-   Slope ($b_1$) has the same sign as the correlation coefficient ($r$): $b_1 = r \\frac{s_Y}{s_X}$\n\n-   The regression line goes through the center of mass point (the coordinates corresponding to average $X$ and average $Y$)\n\n-   Sum of the residuals is zero\n:::\n\n# Application exercise\n\n## ae\\-12\\-modeling\\-penguins {.smaller}\n\n::: appex\n-   Go to your ae project in RStudio.\n\n-   If you haven't yet done so, make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.\n\n-   If you haven't yet done so, click Pull to get today's application exercise file: *ae\\-12\\-modeling\\-penguins\\.qmd*.\n\n-   Work through the application exercise in class, and render, commit, and push your edits.\n:::\n\n# Regression with Categorical Variables\n\n## Regression with Categorical Variables\n\nWhat does $b_0 +b_1*x$ even mean when $x$ is categorical ???\n\n. . .\n\nSuppose variable *island* can take values *A, B,* or *C.* We want to model variable *mass* based on *island*.\n\n. . .\n\nWe tell R to fit `mass ~ island` ... now what?\n\n. . .\n\n[We get ***dummy variables!!!***]{style=\"background-color: #fcefb3\"}\n\n## Dummy Variables\n\n::: columns\n::: {.column width=\"50%\"}\n| mass | island |\n|------|--------|\n| 10   | A      |\n| 30   | C      |\n| 20   | B      |\n| 15   | A      |\n:::\n:::\n\n## Dummy Variables\n\n::: columns\n::: {.column width=\"50%\"}\n| mass | island |\n|------|--------|\n| 10   | A      |\n| 30   | C      |\n| 20   | B      |\n| 15   | A      |\n:::\n\n::: {.column width=\"50%\"}\n| mass | island | **A** | B   | C   |\n|------|--------|-------|-----|-----|\n| 10   | A      |       |     |     |\n| 30   | C      |       |     |     |\n| 20   | B      |       |     |     |\n| 15   | A      |       |     |     |\n:::\n:::\n\n. . .\n\n::: callout-important\nIn a given row, only *one* of the dummy variables for a given categorical variable can equal 1.\n:::\n\n## What will a regression output look like?\n\n$$\nmass = b_0 + b_2 * B + b_3 *C\n$$\n\n. . .\n\nWhat is the estimated weight of a penguin from island *A?*\n\n. . .\n\nWhat is the estimated weight of a penguin from island *B?*\n\n. . .\n\nWhat is the estimated weight of a penguin from island *C?*\n\n. . .\n\n## Let's take a look at the actual penguins data. Back to the AE!\n",
    "supporting": [
      "14-simple-reg_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}