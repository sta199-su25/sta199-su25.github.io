{
  "hash": "0a552190365eb0bcd5d1fe960a092d17",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Logistic regression\"\nsubtitle: \"Lecture 18\"\ndate: \"2025-3-25\"\nformat: revealjs\nauto-stretch: false\n---\n\n## While you wait...\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n::: appex\n-   Go to your `ae` project in RStudio.\n\n-   Make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.\n\n-   Click Pull to get today's application exercise file: *ae\\-14\\-spam\\-filter\\.qmd*.\n\n-   Wait till the you're prompted to work on the application exercise during class before editing the file.\n:::\n\n## Thus far...\n\nWe have been studying regression:\n\n-   What combinations of data types have we seen?\n\n-   What did the picture look like?\n\n\n## Today: a *binary* response {.smaller}\n\n$$\ny = \n\\begin{cases}\n1 & &&\\text{eg. Yes, Win, True, Heads, Success}\\\\\n0 & &&\\text{eg. No, Lose, False, Tails, Failure}.\n\\end{cases}\n$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n## Who cares?\n\nIf we can model the relationship between predictors ($x$) and a binary response ($y$), we can use the model to do a special kind of prediction called *classification*.\n\n## Example: is the e-mail spam or not? {.smaller}\n\n$$\n\\mathbf{x}: \\text{word and character counts in an e-mail.}\n$$\n\n![](images/18/spam.png){fig-align=\"center\" width=\"70%\"}\n\n$$\ny\n= \n\\begin{cases}\n1 & \\text{it's spam}\\\\\n0 & \\text{it's legit}\n\\end{cases}\n$$\n\n## Example: is it cancer or not? {.smaller}\n\n$$\n\\mathbf{x}: \\text{features in a medical image.}\n$$\n\n![](images/18/head-neck.jpg){fig-align=\"center\"}\n\n$$\ny\n= \n\\begin{cases}\n1 & \\text{it's cancer}\\\\\n0 & \\text{it's healthy}\n\\end{cases}\n$$\n\n## Example: will they default? {.smaller}\n\n$$\n\\mathbf{x}: \\text{financial and demographic info about a loan applicant.}\n$$\n\n![](images/18/fico.jpg){fig-align=\"center\" width=\"60%\"}\n\n$$\ny\n= \n\\begin{cases}\n1 & \\text{applicant is at risk of defaulting on loan}\\\\\n0 & \\text{applicant is safe}\n\\end{cases}\n$$\n\n## How do we model this type of data?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n## Straight line of best fit is a little silly\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n## Instead: S-curve of best fit {.smaller}\n\nInstead of modeling $y$ directly, we model the probability that $y=1$:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n-   \"Given new email, what's the probability that it's spam?''\n-   \"Given new image, what's the probability that it's cancer?''\n-   \"Given new loan application, what's the probability that they default?''\n\n## Why don't we model y directly?\n\n-   **Recall regression with a numerical response**:\n\n    -   Our models do not output *guarantees* for $y$, they output predictions that describe behavior *on average*;\n\n-   **Similar when modeling a binary response**:\n\n    -   Our models cannot directly guarantee that $y$ will be zero or one. The correct analog to \"on average\" for a 0/1 response is \"what's the probability?\"\n\n## So, what is this S-curve, anyway?\n\nIt's the *logistic function*:\n\n$$\n\\text{Prob}(y = 1)\n=\n\\frac{e^{\\beta_0+\\beta_1x}}{1+e^{\\beta_0+\\beta_1x}}.\n$$\n\nIf you set $p = \\text{Prob}(y = 1)$ and do some algebra, you get the simple linear model for the *log-odds*:\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right)\n=\n\\beta_0+\\beta_1x.\n$$\n\nThis is called the *logistic regression* model.\n\n## Log-odds?\n\n-   $p = \\text{Prob}(y = 1)$ is a probability. A number between 0 and 1;\n\n-   $p / (1 - p)$ is the odds. A number between 0 and $\\infty$;\n\n> \"The odds of this lecture going well are 10 to 1.\"\n\n-   The log odds $\\log(p / (1 - p))$ is a number between $-\\infty$ and $\\infty$, which is suitable for the linear model.\n\n## Probability to odds\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n## Odds to log odds\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n## Logistic regression\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right)\n=\n\\beta_0+\\beta_1x.\n$$\n\n-   The *logit* function $\\log(p / (1-p))$ is an example of a *link function* that transforms the linear model to have an appropriate range;\n\n-   This is an example of a *generalized linear model*;\n\n## Estimation\n\n-   We estimate the parameters $\\beta_0,\\,\\beta_1$ using *maximum likelihood* (don't worry about it) to get the \"best fitting\" S-curve;\n\n-   The fitted model is\n\n$$\n\\log\\left(\\frac{\\widehat{p}}{1-\\widehat{p}}\\right)\n=\nb_0+b_1x.\n$$\n\n## Today's data {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail |> dplyr::select(c(spam, dollar, viagra, winner, password, exclaim_mess)) |> glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 3,921\nColumns: 6\n$ spam         <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ dollar       <dbl> 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,…\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ winner       <fct> no, no, no, no, no, no, no, no, no, no, no, no,…\n$ password     <dbl> 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ exclaim_mess <dbl> 0, 1, 6, 48, 1, 1, 1, 18, 1, 0, 2, 1, 0, 10, 4,…\n```\n\n\n:::\n:::\n\n\n## Fitting a logistic model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_fit <- logistic_reg() |>\n  fit(spam ~ exclaim_mess, data = email)\n\ntidy(logistic_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term          estimate std.error statistic p.value\n  <chr>            <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)  -2.27      0.0553     -41.1     0    \n2 exclaim_mess  0.000272  0.000949     0.287   0.774\n```\n\n\n:::\n:::\n\n\nFitted equation for the log-odds:\n\n$$\n\\log\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right)\n=\n-2.27\n+\n0.000272\\times exclaim~mess\n$$\n\n## Interpreting the intercept\n\nIf `exclaim_mess = 0`, then\n\n$$\n\\hat{p}=\\widehat{P(y=1)}=\\frac{e^{-2.27}}{1+e^{-2.27}}\\approx 0.09.\n$$\n\nSo, an email with no exclamation marks has a 9% chance of being spam.\n\n## Interpreting the slope is tricky {.scrollable}\n\nRecall:\n\n$$\n\\log\\left(\\frac{\\widehat{p}}{1-\\widehat{p}}\\right)\n=\nb_0+b_1x.\n$$\n\n. . .\n\nAlternatively:\n\n$$\n\\frac{\\widehat{p}}{1-\\widehat{p}}\n=\ne^{b_0+b_1x}\n=\n\\color{blue}{e^{b_0}e^{b_1x}}\n.\n$$\n\n. . .\n\nIf we increase $x$ by one unit, we have:\n\n$$\n\\frac{\\widehat{p}}{1-\\widehat{p}}\n=\ne^{b_0}e^{b_1(x+1)}\n=\ne^{b_0}e^{b_1x+b_1}\n=\n{\\color{blue}{e^{b_0}e^{b_1x}}}{\\color{red}{e^{b_1}}}\n.\n$$\n\n. . .\n\nA one unit increase in $x$ is associated with a change in odds by a factor of $e^{b_1}$. Gross!\n\n## Back to the example...\n\n$$\n\\log\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right)\n=\n-2.27\n+\n0.000272\\times exclaim~mess\n$$\n\nIf we add one exclamation mark to the model, we predict the odds of an email being spam to be **higher** by a factor of $e^{0.000272}\\approx 1.000272$ on average.\n\n## Logistic regression -\\> classification?\n\n## Step 0: fit the model {.smaller}\n\nSelect a number $0 < p^* < 1$:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/step-0-1.png){width=960}\n:::\n:::\n\n\n-   if $\\text{Prob}(y=1)\\leq p^*$, then predict $\\widehat{y}=0$;\n-   if $\\text{Prob}(y=1)> p^*$, then predict $\\widehat{y}=1$.\n\n## Step 1: pick a threshold {.smaller}\n\nSelect a number $0 < p^* < 1$:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/step-1-1.png){width=960}\n:::\n:::\n\n\n-   if $\\text{Prob}(y=1)\\leq p^*$, then predict $\\widehat{y}=0$;\n-   if $\\text{Prob}(y=1)> p^*$, then predict $\\widehat{y}=1$.\n\n## Step 2: find the \"decision boundary\" {.smaller}\n\nSolve for the x-value that matches the threshold:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/step-2-1.png){width=960}\n:::\n:::\n\n\n-   if $\\text{Prob}(y=1)\\leq p^*$, then predict $\\widehat{y}=0$;\n-   if $\\text{Prob}(y=1)> p^*$, then predict $\\widehat{y}=1$.\n\n## Step 3: classify a new arrival {.smaller}\n\nA new person shows up with $x_{\\text{new}}$. Which side of the boundary are they on?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/step-3-1.png){width=960}\n:::\n:::\n\n\n-   if $x_{\\text{new}} \\leq x^\\star$, then $\\text{Prob}(y=1)\\leq p^*$, so predict $\\widehat{y}=0$ for the new person;\n-   if $x_{\\text{new}} > x^\\star$, then $\\text{Prob}(y=1)> p^*$, so predict $\\widehat{y}=1$ for the new person.\n\n## Let's change the threshold {.smaller}\n\nA new person shows up with $x_{\\text{new}}$. Which side of the boundary are they on?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/lower-threshold-1.png){width=960}\n:::\n:::\n\n\n-   if $x_{\\text{new}} \\leq x^\\star$, then $\\text{Prob}(y=1)\\leq p^*$, so predict $\\widehat{y}=0$ for the new person;\n-   if $x_{\\text{new}} > x^\\star$, then $\\text{Prob}(y=1)> p^*$, so predict $\\widehat{y}=1$ for the new person.\n\n## Let's change the threshold {.smaller}\n\nA new person shows up with $x_{\\text{new}}$. Which side of the boundary are they on?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/higher-threshold-1.png){width=960}\n:::\n:::\n\n\n-   if $x_{\\text{new}} \\leq x^\\star$, then $\\text{Prob}(y=1)\\leq p^*$, so predict $\\widehat{y}=0$ for the new person;\n-   if $x_{\\text{new}} > x^\\star$, then $\\text{Prob}(y=1)> p^*$, so predict $\\widehat{y}=1$ for the new person.\n\n## Nothing special about one predictor... {.smaller}\n\nTwo numerical predictors and one binary response:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n## \"Multiple\" logistic regression\n\nOn the probability scale:\n\n$$\n\\text{Prob}(y = 1)\n=\n\\frac{e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_mx_m}}{1+e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_mx_m}}.\n$$\n\nFor the log-odds, a *multiple* linear regression:\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right)\n=\n\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_mx_m.\n$$\n\n## Decision boundary, again {.smaller}\n\nIt's linear! Consider two numerical predictors:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n-   if new $(x_1,\\,x_2)$ below, $\\text{Prob}(y=1)\\leq p^*$. Predict $\\widehat{y}=0$ for the new person;\n-   if new $(x_1,\\,x_2)$ above, $\\text{Prob}(y=1)> p^*$. Predict $\\widehat{y}=1$ for the new person.\n\n## Decision boundary, again {.smaller}\n\nIt's linear! Consider two numerical predictors:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n-   if new $(x_1,\\,x_2)$ below, $\\text{Prob}(y=1)\\leq p^*$. Predict $\\widehat{y}=0$ for the new person;\n-   if new $(x_1,\\,x_2)$ above, $\\text{Prob}(y=1)> p^*$. Predict $\\widehat{y}=1$ for the new person.\n\n## Decision boundary, again {.smaller}\n\nIt's linear! Consider two numerical predictors:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n-   if new $(x_1,\\,x_2)$ below, $\\text{Prob}(y=1)\\leq p^*$. Predict $\\widehat{y}=0$ for the new person;\n-   if new $(x_1,\\,x_2)$ above, $\\text{Prob}(y=1)> p^*$. Predict $\\widehat{y}=1$ for the new person.\n\n## Note: the classifier isn't perfect {.medium}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n-   There are blue points in the orange region: spam (1) emails misclassified as legit (0);\n-   There are orange points in the blue region: legit (0) emails misclassified as spam (1).\n\n## How do you pick the threshold? {.smaller}\n\nTo balance out the two kinds of errors:\n\n![](images/18/confusion-matrix.png)\n\n-   High threshold \\>\\> Hard to classify as 1 \\>\\> FP less likely; FN more likely\n-   Low threshold \\>\\> Easy to classify as 1 \\>\\> FP more likely; FN less likely\n\n## Silly examples\n\n-   Set p\\* = 0\n\n    -   Classify every email as spam (1);\n    -   No false negatives, but *a lot* of false positives;\n\n-   Set p\\* = 1\n\n    -   Classify every email as legit (0);\n    -   No false positives, but *a lot* of false negatives.\n\nYou pick a threshold in between to strike a balance. The exact number depends on context.\n\n## ae\\-14\\-spam\\-filter\n\n::: appex\n-   Go to your ae project in RStudio.\n\n-   If you haven't yet done so, make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.\n\n-   If you haven't yet done so, click Pull to get today's application exercise file: *ae\\-14\\-spam\\-filter\\.qmd*.\n\n-   Work through the application exercise in class, and render, commit, and push your edits.\n:::\n",
    "supporting": [
      "18-logistic_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}