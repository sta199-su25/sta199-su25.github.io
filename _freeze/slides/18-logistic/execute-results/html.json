{
  "hash": "f5848d201957c5b5eebc3dcfbdfde0f1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Logistic regression\"\nsubtitle: \"Lecture 18\"\ndate: \"2025-6-11\"\nformat: \n  revealjs:\n    output-file: 18-logistic-slides.html\n    pdf-separate-fragments: true\nauto-stretch: false\n---\n\n## While you wait... {.smaller}\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n::: appex\n-   Go to your `ae` project in RStudio.\n\n-   Make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.\n\n-   Click Pull to get today's application exercise file: *ae\\-14\\-spam\\-filter\\.qmd*.\n\n-   Wait till the you're prompted to work on the application exercise during class before editing the file.\n:::\n\n## Thus far...\n\nWe have been studying regression:\n\n-   What combinations of data types have we seen?\n\n-   What did the picture look like?\n\n## Linear Models: Just because you can... {.smaller}\n\n... doesn't mean you should!!\n\n. . .\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n## Linear Models: Just because you can... {.smaller}\n\n... doesn't mean you should!!\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n## Linear models have infinite range {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n## Today: a *binary* response {.smaller}\n\nCategorical with **two levels** (0 or 1).\n\n. . .\n\n::: incremental\n-   Yes (1) vs. No (0)\n-   Win (1) vs. Lose (0)\n-   True (1) vs. False (0)\n-   Heads (1) vs. Tails (0)\n-   And so much more!\n:::\n\n. . .\n\n$$\ny = \n\\begin{cases}\n1 & &&\\text{eg. Yes, Win, True, Heads, ...}\\\\\n0 & &&\\text{eg. No, Lose, False, Tails, ...}\n\\end{cases}\n$$\n\n## Example Plot\n\n::: columns\n::: {.column width=\"20%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       x y\n1  -1.00 0\n2   0.72 1\n3  -0.62 0\n4   2.03 1\n5   1.07 1\n6   0.99 1\n7   0.03 1\n8   0.67 1\n9   0.57 1\n10  0.90 1\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"80%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## Who cares?\n\nIf we can model the relationship between predictors ($x$) and a binary response ($y$), we can use the model to do a special kind of prediction called **classification**.\n\n## Example: is the e-mail spam or not? {.smaller}\n\n$$\n\\mathbf{x}: \\text{word and character counts in an e-mail.}\n$$\n\n![](images/18/spam.png){fig-align=\"center\" width=\"70%\"}\n\n$$\ny\n= \n\\begin{cases}\n1 & \\text{it's spam}\\\\\n0 & \\text{it's legit}\n\\end{cases}\n$$\n\n## Example: is it cancer or not? {.smaller}\n\n$$\n\\mathbf{x}: \\text{features in a medical image.}\n$$\n\n![](images/18/head-neck.jpg){fig-align=\"center\"}\n\n$$\ny\n= \n\\begin{cases}\n1 & \\text{it's cancer}\\\\\n0 & \\text{it's healthy}\n\\end{cases}\n$$\n\n## Example: will they default? {.smaller}\n\n$$\n\\mathbf{x}: \\text{financial and demographic info about a loan applicant.}\n$$\n\n![](images/18/fico.jpg){fig-align=\"center\" width=\"60%\"}\n\n$$\ny\n= \n\\begin{cases}\n1 & \\text{applicant is at risk of defaulting on loan}\\\\\n0 & \\text{applicant is safe}\n\\end{cases}\n$$\n\n## How do we model this type of data? {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n## Straight line of best fit is a little silly {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n## Modelling probabilities {.smaller}\n\nInstead of modeling $y$ directly, let's model the probability that $y=1$:\n\n-   \"Given new email, what's the probability that it's spam?''\n-   \"Given new image, what's the probability that it's cancer?''\n-   \"Given new loan application, what's the probability that they default?''\n\n## Modelling probabilities: lines are still silly {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n## Instead: S-curve of best fit {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n## Why don't we model y directly? {.smaller}\n\n-   **Recall regression with a numerical response**:\\\n    \\\n    Our models do not output *guarantees* for $y$, they output predictions that describe behavior *on average*;\n\n. . .\n\n<br>\n\n-   **Similar when modeling a binary response**:\\\n    \\\n    Our models cannot directly guarantee that $y$ will be zero or one. The correct analog to \"on average\" for a 0/1 response is \"what's the probability?\"\n\n## On average vs. What's the probability? {.smaller}\n\nLet's suppose I'm classifying emails as spam ( $y = 1$ ) vs. legit ( $y$ = 0 ).\nAt some given length (suppose $x = 500$ words), I see that:\n\n-   8 emails were spam\n\n-   2 emails were legit\n\n. . .\n\nWhat does it mean to average these together?\n\n. . .\n\n$$\n\\frac{1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 0 + 0}{10} = \\frac{8}{10} = 0.8\n$$\n\n## Again: S-curve of best fit {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n## So, what is this S-curve, anyways? {.smaller}\n\nIt's the *logistic function*:\n\n$$\n\\text{Prob}(y = 1)\n=\n\\frac{e^{\\beta_0+\\beta_1x}}{1+e^{\\beta_0+\\beta_1x}}.\n$$\n\n. . .\n\nIf you set $p = \\text{Prob}(y = 1)$ and do some algebra, you get the simple linear model for the *log-odds*:\n\n. . .\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right)\n=\n\\beta_0+\\beta_1x.\n$$\n\n. . .\n\nThis is called the *logistic regression* model.\n\n## Log-odds? {.smaller}\n\n::: incremental\n-   $p = \\text{Prob}(y = 1)$ is a probability.\n    A number between 0 and 1;\n\n-   $\\frac{p}{(1 - p)}$ is the odds.\n    A number between 0 and $\\infty$;\n\n    -   80% probability an email is spam; 20% an email is legit\n\n    -   The odds an email is spam are 4 to 1\n\n-   The log odds $\\log(\\frac{p}{1 - p})$ is a number between $-\\infty$ and $\\infty$, which is suitable for the linear model.\n:::\n\n. . .\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right)\n=\n\\beta_0+\\beta_1x.\n$$\n\n## Probability to odds\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n## Odds to log odds\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n## Logistic regression {.smaller}\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right)\n=\n\\beta_0+\\beta_1x.\n$$\n\n::: incremental\n-   The *logit* function $\\log(\\frac{p}{1-p})$ is an example of a *link function* that transforms the linear model to have an appropriate range;\n\n-   This is an example of a *generalized linear model*\n:::\n\n## Estimation {.smaller}\n\n::: incremental\n-   We estimate the parameters $\\beta_0,\\,\\beta_1$ using *maximum likelihood* (don't worry about it) to get the \"best fitting\" S-curve;\n\n-   The fitted model is\n:::\n\n. . .\n\n$$\n\\log\\left(\\frac{\\widehat{p}}{1-\\widehat{p}}\\right)\n=\nb_0+b_1x.\n$$\n\n## Today's data {.smaller}\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3,921 Ã— 6\n   spam  dollar viagra winner password exclaim_mess\n   <fct>  <dbl>  <dbl> <fct>     <dbl>        <dbl>\n 1 0          0      0 no            0            0\n 2 0          0      0 no            0            1\n 3 0          4      0 no            0            6\n 4 0          0      0 no            0           48\n 5 0          0      0 no            2            1\n 6 0          0      0 no            2            1\n 7 0          0      0 no            0            1\n 8 0          0      0 no            0           18\n 9 0          0      0 no            0            1\n10 0          0      0 no            0            0\n# â„¹ 3,911 more rows\n```\n\n\n:::\n:::\n\n\n## Fitting a logistic model {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_fit <- logistic_reg() |>\n  fit(spam ~ exclaim_mess, data = email)\n\ntidy(logistic_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 5\n  term          estimate std.error statistic p.value\n  <chr>            <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)  -2.27      0.0553     -41.1     0    \n2 exclaim_mess  0.000272  0.000949     0.287   0.774\n```\n\n\n:::\n:::\n\n\n## Fitting a logistic model {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(logistic_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 5\n  term          estimate std.error statistic p.value\n  <chr>            <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)  -2.27      0.0553     -41.1     0    \n2 exclaim_mess  0.000272  0.000949     0.287   0.774\n```\n\n\n:::\n:::\n\n\n. . .\n\nFitted equation for the log-odds:\n\n$$\n\\log\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right)\n=\n-2.27\n+\n0.000272\\times exclaim~mess\n$$\n\n## Be careful!! {.smaller}\n\nðŸ’–âœ… This is correctâœ…ðŸ’–\n\n$$\n\\log\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right)\n=\n-2.27\n+\n0.000272\\times exclaim~mess\n$$\n\n<br>\n\nâŒ ðŸ›‘These are wrong!\nDo not do this!\nâŒðŸ›‘\n\n$$\n\\widehat{spam}\n=\n-2.27\n+\n0.000272\\times exclaim~mess\n$$\n\n$$\n\\widehat{p}\n=\n-2.27\n+\n0.000272\\times exclaim~mess\n$$\n\n## Interpreting the intercept {.smaller}\n\nPlug in $x = 0$:\n\n$$\n\\log\\left(\\frac{\\widehat{p}}{1-\\widehat{p}}\\right)\n=\nb_0+b_1x.\n$$\n\n. . .\n\nWhen $x = 0$, the estimated probability that $y = 1$ is\n\n$$\n\\hat{p} = \\frac{e^{b_0}}{1+e^{b_0}}\n$$\n\n## Interpreting the intercept: emails {.smaller}\n\nIf `exclaim_mess = 0`, then\n\n$$\n\\hat{p}=\\widehat{P(y=1)}=\\frac{e^{-2.27}}{1+e^{-2.27}}\\approx 0.09.\n$$\n\nSo, we estimate that an email with no exclamation marks has a 9% chance of being spam.\n\n## Interpreting the slope is tricky {.smaller}\n\nRecall:\n\n$$\n\\log\\left(\\frac{\\widehat{p}}{1-\\widehat{p}}\\right)\n=\nb_0+b_1x.\n$$\n\n. . .\n\nAlternatively:\n\n$$\n\\frac{\\widehat{p}}{1-\\widehat{p}}\n=\ne^{b_0+b_1x}\n=\n\\color{blue}{e^{b_0}e^{b_1x}}\n.\n$$\n\n. . .\n\nIf we increase $x$ by one unit, we have:\n\n$$\n\\frac{\\widehat{p}}{1-\\widehat{p}}\n=\ne^{b_0}e^{b_1(x+1)}\n=\ne^{b_0}e^{b_1x+b_1}\n=\n{\\color{blue}{e^{b_0}e^{b_1x}}}{\\color{red}{e^{b_1}}}\n.\n$$\n\n. . .\n\nA one unit increase in $x$ is associated with a change in odds by a factor of $e^{b_1}$.\nGross!\n\n## Sign of the slope is meaningful {.smaller}\n\nA one unit increase in $x$ is associated with a change in odds by a factor of $e^{b_1}$.\n\n. . .\n\n-   A positive slope means increasing $x$ increases the odds (and probability!) that $y = 1$\n-   A negative slope means increasing $x$ decreases the odds (and probability!) that $y = 1$\n\n## Back to the example... {.smaller}\n\n$$\n\\log\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right)\n=\n-2.27\n+\n0.000272\\times exclaim~mess\n$$\n\nIf we add one exclamation mark to the model, we predict the odds of an email being spam to be **higher** by a **factor** of $e^{0.000272}\\approx 1.000272$ on average.\n\n# Logistic regression -\\> classification?\n\n## Step 0: fit the model {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/step-0-1.png){width=960}\n:::\n:::\n\n\n## Step 1: pick a threshold {.smaller}\n\nSelect a number $0 < p^* < 1$:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/step-1-1.png){width=960}\n:::\n:::\n\n\n-   if $\\text{Prob}(y=1)\\leq p^*$, then predict $\\widehat{y}=0$;\n-   if $\\text{Prob}(y=1)> p^*$, then predict $\\widehat{y}=1$.\n\n## Step 2: find the \"decision boundary\" {.smaller}\n\nSolve for the x-value that matches the threshold:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/step-2-1.png){width=960}\n:::\n:::\n\n\n-   if $\\text{Prob}(y=1)\\leq p^*$, then predict $\\widehat{y}=0$;\n-   if $\\text{Prob}(y=1)> p^*$, then predict $\\widehat{y}=1$.\n\n## Step 3: classify a new arrival {.smaller}\n\nA new person shows up with $x_{\\text{new}}$.\nWhich side of the boundary are they on?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/step-3-1.png){width=960}\n:::\n:::\n\n\n-   if $x_{\\text{new}} \\leq x^\\star$, then $\\text{Prob}(y=1)\\leq p^*$, so predict $\\widehat{y}=0$ for the new person;\n-   if $x_{\\text{new}} > x^\\star$, then $\\text{Prob}(y=1)> p^*$, so predict $\\widehat{y}=1$ for the new person.\n\n## Let's change the threshold {.smaller}\n\nA new person shows up with $x_{\\text{new}}$.\nWhich side of the boundary are they on?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/lower-threshold-1.png){width=960}\n:::\n:::\n\n\n-   if $x_{\\text{new}} \\leq x^\\star$, then $\\text{Prob}(y=1)\\leq p^*$, so predict $\\widehat{y}=0$ for the new person;\n-   if $x_{\\text{new}} > x^\\star$, then $\\text{Prob}(y=1)> p^*$, so predict $\\widehat{y}=1$ for the new person.\n\n## Let's change the threshold {.smaller}\n\nA new person shows up with $x_{\\text{new}}$.\nWhich side of the boundary are they on?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/higher-threshold-1.png){width=960}\n:::\n:::\n\n\n-   if $x_{\\text{new}} \\leq x^\\star$, then $\\text{Prob}(y=1)\\leq p^*$, so predict $\\widehat{y}=0$ for the new person;\n-   if $x_{\\text{new}} > x^\\star$, then $\\text{Prob}(y=1)> p^*$, so predict $\\widehat{y}=1$ for the new person.\n\n## Nothing special about one predictor... {.smaller}\n\nTwo numerical predictors and one binary response:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-24-1.png){width=960}\n:::\n:::\n\n\n## \"Multiple\" logistic regression {.smaller}\n\nOn the probability scale:\n\n$$\n\\text{Prob}(y = 1)\n=\n\\frac{e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_mx_m}}{1+e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_mx_m}}.\n$$\n\n. . .\n\nFor the log-odds, a *multiple* linear regression:\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right)\n=\n\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_mx_m.\n$$\n\n## Decision boundary, again {.smaller}\n\nConsider two numerical predictors:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n-   if new $(x_1,\\,x_2)$ below, $\\text{Prob}(y=1)\\leq p^*$. Predict $\\widehat{y}=0$ for the new person;\n-   if new $(x_1,\\,x_2)$ above, $\\text{Prob}(y=1)> p^*$. Predict $\\widehat{y}=1$ for the new person.\n\n## Decision boundary, again {.smaller}\n\nIt's linear!\nConsider two numerical predictors:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n\n-   if new $(x_1,\\,x_2)$ below, $\\text{Prob}(y=1)\\leq p^*$. Predict $\\widehat{y}=0$ for the new person;\n-   if new $(x_1,\\,x_2)$ above, $\\text{Prob}(y=1)> p^*$. Predict $\\widehat{y}=1$ for the new person.\n\n## Decision boundary, again {.smaller}\n\nIt's linear!\nConsider two numerical predictors:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-27-1.png){width=960}\n:::\n:::\n\n\n-   if new $(x_1,\\,x_2)$ below, $\\text{Prob}(y=1)\\leq p^*$. Predict $\\widehat{y}=0$ for the new person;\n-   if new $(x_1,\\,x_2)$ above, $\\text{Prob}(y=1)> p^*$. Predict $\\widehat{y}=1$ for the new person.\n\n## Note: the classifier isn't perfect {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-logistic_files/figure-revealjs/unnamed-chunk-28-1.png){width=960}\n:::\n:::\n\n\n-   Blue points in the orange region: spam (1) emails misclassified as legit (0);\n-   Orange points in the blue region: legit (0) emails misclassified as spam (1).\n\n## How do you pick the threshold? {.smaller}\n\nTo balance out the two kinds of errors:\n\n![](images/18/confusion-matrix.png)\n\n::: incremental\n-   High threshold:\n    -   Hard to classify as 1, so FP less likely and FN more likely\n-   Low threshold:\n    -   Easy to classify as 1, so FP more likely and FN less likely\n:::\n\n## Silly examples {.smaller}\n\n-   Set p\\* = 0\n\n    -   Classify every email as spam (1);\n    -   No false negatives, but *a lot* of false positives;\n\n-   Set p\\* = 1\n\n    -   Classify every email as legit (0);\n    -   No false positives, but *a lot* of false negatives.\n\nYou pick a threshold in between to strike a balance.\nThe exact number depends on context.\n\n## ae\\-14\\-spam\\-filter\n\n::: appex\n-   Go to your ae project in RStudio.\n\n-   If you haven't yet done so, make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.\n\n-   If you haven't yet done so, click Pull to get today's application exercise file: *ae\\-14\\-spam\\-filter\\.qmd*.\n\n-   Work through the application exercise in class, and render, commit, and push your edits.\n:::\n",
    "supporting": [
      "18-logistic_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}