{
  "hash": "acd1d2d98845a1e302e61a804303c9be",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hypothesis Testing\"\nsubtitle: \"Lecture 21\"\ndate: \"2025-6-16\"\nformat: \n  revealjs:\n    output-file: 21-hypothesis-test-slides.html\n    pdf-separate-fragments: true\nauto-stretch: false\n---\n\n## While you wait... {.smaller}\n\n\n\n::: appex\n-   Go to your `ae` project in RStudio.\n\n-   Make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.\n\n-   Click Pull to get today's application exercise file: *ae\\-17\\-hypothesis\\-testing\\.qmd*.\n\n-   Wait till the you're prompted to work on the application exercise during class before editing the file.\n:::\n\n# Recap: sampling uncertainty\n\n## What if this was my dataset?\n\n::: columns\n::: {.column width=\"49%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](21-hypothesis-test_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)    2.56 \n2 log_inc        0.718\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"49%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](21-hypothesis-test_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## What if this was my dataset instead?\n\n::: columns\n::: {.column width=\"49%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](21-hypothesis-test_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   -0.250\n2 log_inc        0.964\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"49%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](21-hypothesis-test_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## What if this was my dataset instead?\n\n::: columns\n::: {.column width=\"49%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](21-hypothesis-test_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)    0.468\n2 log_inc        0.885\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"49%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](21-hypothesis-test_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## Rinse and repeat 1000 times...\n\n## Sampling uncertainty {.medium}\n\n\n::: {.cell}\n\n:::\n\n\n::: columns\n::: {.column width=\"49%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](21-hypothesis-test_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n:::\n\n::: {.column width=\"49%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](21-hypothesis-test_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\nHow sensitive are the estimates to the data they are based on?\n\n```         \n- Very? Then uncertainty is high, results are unreliable;\n- Not very? Uncertainty is low, results are more reliable.\n```\n\n## That was for n = 50. What if I was starting with n = 1000?\n\n## Sampling uncertainty decreased!\n\n\n::: {.cell}\n\n:::\n\n\n::: columns\n::: {.column width=\"49%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](21-hypothesis-test_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n:::\n\n::: {.column width=\"49%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](21-hypothesis-test_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## Bootstrapping {.smaller}\n\n::: incremental\n-   Data collection is costly, so we have to do our best with what we already have;\n\n-   We approximate this idea of \"alternative, hypothetical datasets I could have observed\" by resampling our data *with replacement*;\n\n-   We construct a new dataset of the same size by randomly picking rows out of the original one:\n\n    -   Some rows will be duplicated;\n    -   Some rows will not appear at all;\n    -   The new dataset is different from the original;\n    -   Different dataset \\>\\> different estimate\n\n-   Repeat this processes hundred or thousands of times, and observe how the estimates vary as you refit the model on alternative datasets.\n\n-   This gives you a sense of the sampling variability of your estimates.\n:::\n\n## Bootstrapping Procedure\n\n![](images/bootstrap/1.png){fig-align=\"center\"}\n\n## Bootstrapping\n\n![](images/bootstrap/2.png){fig-align=\"center\"}\n\n## Bootstrapping\n\n![](images/bootstrap/3.png){fig-align=\"center\"}\n\n## Bootstrapping\n\n![](images/bootstrap/4.png){fig-align=\"center\"}\n\n## Bootstrapping\n\n![](images/bootstrap/5.png){fig-align=\"center\"}\n\n## Bootstrapping\n\n![](images/bootstrap/6.png){fig-align=\"center\"}\n\n## Bootstrapping\n\n![](images/bootstrap/7.png){fig-align=\"center\"}\n\n## Confidence intervals {.smaller}\n\n::: incremental\n-   **Point estimation**: report your single number *best guess* for the unknown quantity;\n\n-   **Interval estimation**: report a range, or interval, or values where you think the unknown quantity is likely to live;\n\n    -   Interval should be wide enough to capture the truth with high probability;\n    -   Interval should be narrow enough to be informative;\n\n-   Unfortunately, there is a trade-off.\n    You adjust the **confidence level** to try to negotiate the trade-off;\n\n-   Common choices: 90%, 95%, 99%.\n:::\n\n## Precision vs. accuracy {.smaller}\n\n![](images/21/garfield.png)\n\n# Recap: Computing Confidence Interval\n\n## Friday's Data: Houses in Duke Forest {.smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n-   Data on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020\n-   Scraped from Zillow\n-   Source: [`openintro::duke_forest`](http://openintrostat.github.io/openintro/reference/duke_forest.html)\n:::\n\n::: {.column width=\"50%\"}\n![](images/21/duke_forest_home.jpg){fig-alt=\"Home in Duke Forest\"}\n:::\n:::\n\n**Goal**: Use the area (in square feet) to understand variability in the price of houses in Duke Forest.\n\n## Point Estimate {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobserved_fit <- duke_forest |>\n  specify(price ~ area) |>\n  fit()\n\nobserved_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  term      estimate\n  <chr>        <dbl>\n1 intercept  116652.\n2 area          159.\n```\n\n\n:::\n:::\n\n\n## Slopes of bootstrap samples {.smaller}\n\n::: task\n**Fill in the blank:** For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by between \\_\\_\\_ and \\_\\_\\_ dollars.\n:::\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](21-hypothesis-test_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n## Slopes of bootstrap samples {.smaller}\n\n::: task\n**Fill in the blank:** For each additional square foot, we expect the sale price of Duke Forest houses to be higher, on average, by between \\_\\_\\_ and \\_\\_\\_ dollars.\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](21-hypothesis-test_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n## 95% confidence interval {.xsmall}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](21-hypothesis-test_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n::: incremental\n-   A 95% confidence interval is bounded by the middle 95% of the bootstrap distribution\n-   We are 95% confident that for each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $90.17 to $215.39.\n:::\n\n## Where do the bounds come from? {.smaller}\n\n::: incremental\n-   Think IQR!\n    50% of the bootstrap distribution is between the 25% quantile on the left and the 75% quantile on the right.\n    But we want more than 50%\n\n-   90% of the bootstrap distribution is between the 5% quantile on the left and the 95% quantile on the right;\n\n-   95% of the bootstrap distribution is between the 2.5% quantile on the left and the 97.5% quantile on the right;\n\n-   And so on.\n:::\n\n## Recap (again!!) {.xsmall .scrollable}\n\n::: incremental\n-   **Population:** Complete set of observations of whatever we are studying, e.g., people, tweets, photographs, etc. (population size = $N$)\n\n-   **Sample:** Subset of the population, ideally random and representative (sample size = $n$)\n\n-   Sample statistic $\\ne$ population parameter, but if the sample is good, it can be a good estimate\n\n-   **Statistical inference:** Discipline that concerns itself with the development of procedures, methods, and theorems that allow us to extract meaning and information from data that has been generated by stochastic (random) process\n\n-   We report the estimate with a confidence interval, and the width of this interval depends on the variability of sample statistics from different samples from the population\n\n-   Since we can't continue sampling from the population, we bootstrap from the one sample we have to estimate sampling variability\n:::\n\n# Hypothesis testing\n\n## Hypothesis testing {.smaller}\n\nA hypothesis test is a statistical technique used to evaluate *competing claims* using data\n\n::: incremental\n-   **Null hypothesis,** $H_0$: An assumption about the population.\n    \"There is nothing going on.\"\n\n-   **Alternative hypothesis,** $H_A$: A research question about the population.\n    \"There is something going on\".\n:::\n\n. . .\n\nNote: Hypotheses are always at the population level!\n\n## Setting hypotheses {.smaller}\n\n-   **Null hypothesis,** $H_0$: \"There is nothing going on.\" The slope of the model for predicting the prices of houses in Duke Forest from their areas is 0, $\\beta_1 = 0$.\n\n-   **Alternative hypothesis,** $H_A$: \"There is something going on\".\n    The slope of the model for predicting the prices of houses in Duke Forest from their areas is different than, $\\beta_1 \\ne 0$.\n\n## Hypothesis testing \"mindset\" {.smaller}\n\n-   Assume you live in a world where null hypothesis is true: $\\beta_1 = 0$.\n\n-   Ask yourself how likely you are to observe the sample statistic, or something even more extreme, in this world: $P(b_1 \\leq 159.48~or~b_1 \\geq 159.48 | \\beta_1 = 0)$ = ?\n\n## Hypothesis testing as a court trial {.smaller}\n\n-   **Null hypothesis**, $H_0$: Defendant is innocent\n\n-   **Alternative hypothesis**, $H_A$: Defendant is guilty\n\n. . .\n\n-   **Present the evidence:** Collect data\n\n. . .\n\n-   **Judge the evidence:** \"Could these data plausibly have happened by chance if the null hypothesis were true?\"\n    -   Yes: Fail to reject $H_0$\n    -   No: Reject $H_0$\n\n## Hypothesis testing as medical diagnosis {.smaller}\n\n-   **Null hypothesis**, $H_0$: patient is fine\n\n-   **Alternative hypothesis**, $H_A$: patient is sick\n\n. . .\n\n-   **Present the evidence:** Collect data\n\n. . .\n\n-   **Judge the evidence:** \"Could these data plausibly have happened by chance if the null hypothesis were true?\"\n    -   Yes: Fail to reject $H_0$\n    -   No: Reject $H_0$\n\n## Hypothesis testing framework {.smaller}\n\n::: incremental\n-   Start with a null hypothesis, $H_0$, that represents the status quo\n\n-   Set an alternative hypothesis, $H_A$, that represents the research question, i.e. what we're testing for\n\n-   Conduct a hypothesis test under the assumption that the null hypothesis is true and calculate a **p-value** (probability of observed or more extreme outcome given that the null hypothesis is true)\n\n    -   if the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\n    -   if they do, then reject the null hypothesis in favor of the alternative\n:::\n\n## Calculate observed slope\n\n... which we have already done:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobserved_fit <- duke_forest |>\n  specify(price ~ area) |>\n  fit()\n\nobserved_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  term      estimate\n  <chr>        <dbl>\n1 intercept  116652.\n2 area          159.\n```\n\n\n:::\n:::\n\n\n## Simulate null distribution\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1|2|3|4|5|6\"}\nset.seed(20241118)\nnull_dist <- duke_forest |>\n  specify(price ~ area) |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 100, type = \"permute\") |>\n  fit()\n```\n:::\n\n\n## View null distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_dist\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 200 × 3\n# Groups:   replicate [100]\n   replicate term        estimate\n       <int> <chr>          <dbl>\n 1         1 intercept 547294.   \n 2         1 area           4.54 \n 3         2 intercept 568599.   \n 4         2 area          -3.13 \n 5         3 intercept 561547.   \n 6         3 area          -0.593\n 7         4 intercept 526286.   \n 8         4 area          12.1  \n 9         5 intercept 651476.   \n10         5 area         -33.0  \n# ℹ 190 more rows\n```\n\n\n:::\n:::\n\n\n## Visualize null distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_dist |>\n  filter(term == \"area\") |>\n  ggplot(aes(x = estimate)) +\n  geom_histogram(binwidth = 15)\n```\n\n::: {.cell-output-display}\n![](21-hypothesis-test_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n## Visualize null distribution (alternative)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisualize(null_dist) +\n  shade_p_value(obs_stat = observed_fit, direction = \"two-sided\")\n```\n\n::: {.cell-output-display}\n![](21-hypothesis-test_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n## Get p-value\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_dist |>\n  get_p_value(obs_stat = observed_fit, direction = \"two-sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  term      p_value\n  <chr>       <dbl>\n1 area            0\n2 intercept       0\n```\n\n\n:::\n:::\n\n\n## Make a decision\n\n::: task\nBased on the p-value calculated, what is the conclusion of the hypothesis test?\n:::\n\n## Sometimes the test will be wrong {.smaller}\n\n![](images/21/general.png)\n\n## Think about the judge {.smaller}\n\n$H_0$ person innocent vs $H_A$ person guilty\n\n![](images/21/judge.png)\n\n## Think about the doctor {.smaller}\n\n$H_0$ person well vs $H_A$ person sick.\n\n![](images/21/doctor.png)\n\n## How do we negotiate the trade-off? {.smaller}\n\n::: incremental\nPick a threshold $\\alpha\\in[0,\\,1]$ called the **discernibility level** and threshold the $p$-value:\n\n-   If $p\\text{-value} < \\alpha$, reject null and accept alternative;\n-   If $p\\text{-value} \\geq \\alpha$, fail to reject null;\n:::\n\n. . .\n\n![](images/21/alpha.png)\n",
    "supporting": [
      "21-hypothesis-test_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}