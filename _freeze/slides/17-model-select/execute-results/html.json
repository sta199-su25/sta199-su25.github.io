{
  "hash": "20ce9ba246dc00b2f5d376a1f5e5a156",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model Selection\"\nsubtitle: \"Lecture 17\"\ndate: \"2025-06-10\"\nformat: \n  revealjs:\n    output-file: 17-model-select-slides.html\n    pdf-separate-fragments: true\n---\n\n\n\n\n\n## While you wait... {.smaller}\n\nOpen up/keep working on AE13!\n(Goal: fit the interaction effects model).\n\n## Announcements {.smaller}\n\n-   Office hours today: 3:30 - 5:30\n\n-   Lab 4 Q8 part d: challenge!\n\n-   Midterm scores are posted on gradescope: come by office hours or schedule a meeting with me to see your in-class exam!\n\n## Grading Reminders{.smaller}\n\n-   Labs are equally weighted and the lowest is dropped\n\n-   Midterm is worth 20% of your final grade\n\n-   Final is worth 20% of your final grade\n\n-   ***New policy:** If you score better on the final exam than the midterm, we will weight the final exam higher in your final grade calculation.*\n\n# Let us help you!!!\n\nNo question is too big or too small.\nSeriously!!\n\n## So many models...{.smaller}\n\n-   We've seen a bunch of different linear models for predicting continuous outcomes\n\n-   How can we tell how good they are?\n    How can we choose between models?\n\n## Recall: Residuals{.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](17-model-select_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Recall: Least squares line{.smaller}\n\nThe residual for the $i^{th}$ observation is\n\n$$e_i = \\text{observed} - \\text{predicted} = y_i - \\hat{y}_i$$\n\n. . .\n\nThe sum of squared residuals is\n\n$$e^2_1 + e^2_2 + \\dots + e^2_n$$\n\n. . .\n\nThe least squares line is the one that minimizes the sum of squared residuals\n\n## The concept still applies!{.smaller}\n\n-   Each model we fit since simple linear regression has minimized a sum of squared residuals.\n\n. . .\n\n-   Even if we have more than one $x$ (explanatory variable), we still predict one $\\hat{Y}$.\n\n. . .\n\n-   The residual for the $i^{th}$ observation is still\n\n$$e_i = \\text{observed} - \\text{predicted} = y_i - \\hat{y}_i$$ \n\n\n. . . \n\n-   Reminder: use `augment` to get predictions and residuals\n\n## Example{.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](17-model-select_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n## How do we pick between our models? {.smaller}\n\n-   Over the past few days, we have fit a ton of linear models to predict penguins' body mass.\n\n    -   Simple linear regression with flipper length\n\n    -   Regression with island\n\n    -   Additive model with island and flipper length\n\n    -   Interaction model with island and flipper length\n\n. . .\n\n-   Wouldn't it be nice if there was some score that would tell us which is the \"best\"??\n\n## Recall: R (no, not the language) {.smaller}\n\n::: incremental\n-   Recall: Correlation (R) is a value between -1 and 1 that tells us how strong of a linear relationship two variables haves\n-   Now: $R^2$ (between 0 and 1) gives the *proportion of variability in the outcome explained by the model*\n-   $R^2$ is useful for quantifying the fit of a given model...\n-   ... but $R^2$ *always goes up* every time you add *any* predictor to a model, even if that predictor is silly and useless.\n:::\n\n## Adjusted $R^2$ {.smaller}\n\n::: incremental\n-   Adjusted $R^2$ is an... adjusted version of $R^2$ that penalized the number of predictors in the model\n-   This makes it very useful for comparing models!\n-   Philosophically, we want a model that *both:*\n    -   Fits/predicts well *and...*\n    -   Is as simple as possible (so we can understand it)\n-   ***Idea: Occam's razor -*** when faced with many explanations/models, we should choose the least complex one that fits the data well.\n:::\n\n## How to implement? {.smaller}\n\nUse the function `glance` to compute $R^2$ and adjusted $R^2$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(bm_fl_island_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl>\n1     0.774         0.772  383.      386. 7.60e-109     3 -2517. 5045.\n# ℹ 4 more variables: BIC <dbl>, deviance <dbl>, df.residual <int>,\n#   nobs <int>\n```\n\n\n:::\n:::\n\n## How to compare?{.smaller}\n\nJust flipper length: \n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl>\n1     0.759         0.758  394.     1071. 4.37e-107     1 -2528. 5063.\n# ℹ 4 more variables: BIC <dbl>, deviance <dbl>, df.residual <int>,\n#   nobs <int>\n```\n\n\n:::\n:::\n\n\nJust island : \n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl>\n1     0.394         0.390  626.      110. 1.52e-37     2 -2686. 5380.\n# ℹ 4 more variables: BIC <dbl>, deviance <dbl>, df.residual <int>,\n#   nobs <int>\n```\n\n\n:::\n:::\n\n## How to compare?{.smaller}\n\nAdditive model:\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl>\n1     0.774         0.772  383.      386. 7.60e-109     3 -2517. 5045.\n# ℹ 4 more variables: BIC <dbl>, deviance <dbl>, df.residual <int>,\n#   nobs <int>\n```\n\n\n:::\n:::\n\n\nInteraction model:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl>\n1     0.786         0.783  374.      246. 4.55e-110     5 -2508. 5031.\n# ℹ 4 more variables: BIC <dbl>, deviance <dbl>, df.residual <int>,\n#   nobs <int>\n```\n\n\n:::\n:::\n\n\n# A note about linear regression\nThink about in your projects!\n\n## Why use more variables?{.smaller}\n\n:::incremental\n- Real-world outcomes are influenced by **many factors**\n- Multiple regression lets us:\n  - Include more explanatory variables\n  - **Control for** variables that might confound relationships\n- This helps isolate the **effect of interest**\n:::\n\n\n## Example: Predicting Wages {.smaller}\n\nYou want to understand how **gender** affects wages.\n\n```r\nwage ~ gender\n```\n\nThis tells you the **overall difference**, but doesn't control for **education** or **experience**.\n\n---\n\n## Add control variables {.smaller}\n\n```r\nwage ~ gender + education + experience\n```\n\nNow you're comparing:\n\n> People with the **same education and experience**  \n> But different gender\n\nMore fair, more realistic, more useful!\n\n---\n\n## Key idea: Controlling for a variable {.smaller}\n\n> Holding a variable **constant** to isolate the effect of another\n\nYou're asking:\n\n> \"What is the effect of gender **if education and experience are the same**?\"\n\nThis is what we mean by **controlling for** variables.\n\n---\n\n## Why this matters {.smaller}\n\nWithout controlling, we might:\n\n:::incremental\n- Overstate or understate effects\n- Confuse correlation with causation\n- Miss important patterns\n\n:::\n\n. . . \n\nMultiple regression gives us a better picture of **how the world really works**.\n\n\n## Takeaway {.smaller}\n\nWhen doing data analysis, think about what variables might be related to both your outcome and your explanatory variables!\n\n# Projects\n\n## Talk about:\n\n-  Milestone 2 feedback\n\n-  Milestone 3/4/5 requirements\n\n## Key reminders:\n\n-  use `#| echo: false` in code chunks\n\n-  make sure the website renders!!!!\n\n-  READ THE REQUIREMENTS CAREFULLY!!!\n\n",
    "supporting": [
      "17-model-select_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}